{
  

  
  
      "page0": {
          "title": "Data",
          "content": "This project is built on three sources of data. A detailed description of each dataset will be described in the following three sections. . Game Of Thrones Wikipage . The first data source is the Game Of Thrones Wikipage, where it was possible to extract the data using their API. The Wiki-page contains a section describing each season of the series, where this text contains information of characters being involved in each season. Through the use of regular expressions it was possible to extract characters present in each season. Having extracted the characters present in the series we could again utilize the API to extract and gather information about each character and save these text files. . An example can be seen here: link to character page. Information included in a character page are an image of the character, some basic information including: seasons where the character are present, religion, allegiance, culture, family information, status ie. whether the character are dead or alive. It should be noted that the available information can vary between the characters. . Further information are bibliography, family tree etc. but again it should be noted that the available information can vary heavily. The character pages can be used to find interactions between the characters, but also gather basic information about each character. . The wikipage was used to gather information about each character ie. basic stats such as religion, status, appearances, allegiance but also interactions between the characters. This will be further explained and investigated in the coming pages. Furthermore, the character pages were used to investigate words used by the character and also the sentiment of these wiki-pages. . Another approach could be to investigate all the possible wikipages, but this would involve scraping 4723 pages (at the moment), where a lot of these pages are describing scenes, places and eg. religions, which is not important information about the characters. . The extracted data from the wikipages can be found here the text for each character across all season can be downloaded using this link which is raw text and the initial cleaned text here. Extracted data on seasonal level can be found here. . The code used to extract information from the wikipages can be found in the Explainer Notebook. . Character dialogoues . The second data source are character dialogoues which can be found here. The character dialogoues are based on transcripts of the series&#39; episodes, and as all transcripts are not freely available some of them are created by fans. The data are divided into dialogoues for each episode and which character the dialogue belong to. . The data are already cleaned and ready to use. The data are used for text- and sentiment analysis later in this project. . . IMDB data . The last data source are reviews and ratings from the IMDB database. The data are extracted using a python package IMDbPY, which contains the average ratings of each episode based on the IMDB database. The ratings does also contain demographic information such as the age and gender of the ratings. . It is also possible to extract reviews of each episode but only a limited amount of reviews can be extracted for each episode namely 25 as maximum. Another approach could be to scrape the IMDB website, but according to the IMDB webpage this is not permitted. Therefore the limited amount of reviews pr. episode are used. The reviews also contain a time-stamp, user-id and rating, but these ratings does not appear to be correct as these are either 0 or 1, which does not correspond to the either the average ratings of the episodes or the text corresponding to the ratings. . The idea is to investigate whether the reivewers text correspond with the average rating of the episodes and also whether the ratings/reviews changes throughout the season/episodes. This is also wished to compare with the sentiments of the character, but more on this later in the project. . The gathered data can be found here link. . . All data used for creating network and perform text analysis can be found in this folder. .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/data/",
          "relUrl": "/data/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Basic Statistics",
          "content": "This part of the webpage are used to give a further introduction to the data used for analysis in this project. The Data introduces the basics of the data, whereas this page will dive further in to what the data contains and some of its properties. A full analysis can be found in the Explainer Notebook. . This page will contain three parts: firstly, properties of the Game Of Thrones network are presented, secondly properties of the dialogoues are presented and lastly the reviews and ratings from IMDB. . Game Of Thrones Network . This part are used to introduce properties of the data used for network analysis of the data especially there are focus on the properties of the characters namely their religion, allegiance, culture, number of appearances in the series and status ie. whether they are dead or alive at the end of the series. . The network data contains 162 characters, which is found by scraping the gameofthrones.fandom.com webpage. Each character has been assigned a number of attributes namely relgion, allegiance, culture, status ie. whether the character is dead or alive and lastly the number of appearances the character has through the series. . We will start out by investigating how the characters are distributed across these attributes, are some of groups of each attribute more frequent than others. This is going to be presented in interactive figures where the categories of each attribute can be selected and deselected by clicking these on and off. If ones want to only investigate one category this can be done by double-clicking the category of interest. . Let&#39;s start out with how the characters distribute across the different religions. . . . From this it is apparent that a lot of the characters does not have a known religion, which is the majority of the characters, but if we toggle this of, we can see that the majority of the characters are part of Faith of The Seven and Old Gods of the Forest. On the other side it should be noted that the least frequent religions are White Walkers and Ghiscari religion. From the basic knowledge of the Game Of Thrones universe it also makes sense that the two most popular religions are Faith of The Seven and Old Gods of the Forest as the The Seven Kingdoms are practicing the Faith of the Seven whereas the people in the North are practicing the Old Gods of the Forest. . Further it should be noted that the the Game Of Thrones universe contains 8 different religions based on the Wiki pages. . Next, we are going to investigate the frequencies of the different allegiances. The allegiances are a strong factor in Game Of Thrones as this has a large effect in the wars, and how people interact. How the different houses talk and interact are strongly affected by the allegiances. Some allegiances are known to be hostile to each other such as the House of Stark and House of Lannister. But also House Baratheon of King&#39;s Landing are known to be very hostile against House Targaryen, and these two Houses are known to be in war due to past history where the Mad King did kill people for fun. . . . Again, some of the characters does not have an associated allegiance. The two most frequent allegiances are House Stark and Hose Lannister, followed by Night&#39;s Watch and House Targaryen. These allegiances are also the main allegiances in Game Of Thrones and further also the allegiances of the main characters in the series. . House Lannister has characters as Cersei, Jamie and Tyrion whereas House Stark has Robb, Bran and the bastard Jon Snow. Jon Snow is one of the series most well known character which is also part of the Night&#39;s Watch, and the the Night&#39;s Watch are playing a big role later in the series when the battle against the White Walkers are happening. Lastly, House Targaryen are a house which is beaten down but as the series are evolving Daenerys are becoming a larger player in the universe as she conquers the world part by part. . The characters are not only divided into allegiances, but also cultures, which has shown to be important. The people in the North are helping each other out even though they are not part of the same allegiance. . . . From the above figure it can be seen that the most prominent culture are Andals followed by Northmen, again a large group has a unknown culture. From this it is apparent that most of the characters are found in the Andals and Northmen cultures, and makes the majority of the Game Of Thrones universe. Further, it should be noted that the universe contains a lot of small cultures such Children of the Forest. . The Andals are the people who invaded Westeros in the beginning of the universe, and are the dominant group. The Northmen are also a big cultural group defined by all the characters living in the North of the Game Of Thrones world. The Children of the Forest are a small group of characters which are presented fairly late in the series. They are small non-human characters, and should be the original people of Westeros. Further it should be noticed that the network contains a lot of different cultures. . It is further investigated how many of the characters that die through the series. We start out with 224 characters, and end up with only 30 characters being alive, whereas 8 is uncertain and 2 unkown. . This means that 121 characters dies throughout the series, and anyone who has seen the series would be able to confirm that a lot of characters die as the series progresses. In the figure below the distribution of the characters status can be seen. . . . Next we will dive into the last attribute for each character in the network, namely how many appearances the character has throughout the series. This will give us indication how often we in general will see a character but also present if there are any strict patterns. . . . Again, a lot of characters do not have this attribute on their character page, and these observations have been omitted in the figure above. We can see that the majority of the characters only appear a couple of times ie. below 10-15 apperances. This would make sense as a lot of the characters are not main characters and therefore only appear in a season or likewise. We can further see a little group around 40 appearances and 60 appearances which could indicate we have a little group of characters appearing in most episodes, which would be expected as the series have a couple of main characters. . Character dialogoues . Next we dive into the character dialogoues which are extracted from transcripts, this dataset contains dialogoues from all characters in the season, and this is based on another dataset than in the previous part of this page. Therefore we restrict the data to only contain data for the characters that are present in the network used for analysis in Text Analysis. . Originally the data contains 817 characters and the original dataset can be found here. . We are going to investigate how many episodes and series does each character appear in and also what is the average token length ie. how much dialogoue are present for each character as the dialogoue length could indicate the importance of a character. This is thought as a good approximation, as a character with a lot of dialogoue probably also are present a lot in the series and this could indicate the importance of the character. . . . From the figure above we can see that Tyrion Lannister clearly are the character with the longest dialogoue, which for anyone who has seen the series knows that Tyrion talks a lot and likes to talk. Next we can see that Jon Snow, Cersei Lannister and Daenerys Targaryen also has a lot of dialogoue. This makes sense as these three are part of the main characters, and appear in a lot of episodes. . Next we will dive into the characters appearances in seasons but also episodes, this is done by finding the episodes and series where they have some diaologoue and use this as indications of appearance. . . . From the figure above it can be seen that a lot of characters are present in all 8 season such as: Jon Snow, Sansa Stark, Tyrion Lannister, Bronn and Samwell Tarly and again this is expected as these characters are part of the key characters. On the other side a lot of characters are only present in 1 season such as Syrio Forel which is Arya Starks &quot;dancing teacher&quot; when she moves to King&#39;s Landing. . We will now investigate the appearance on episode level as this can give a more fine coarsed description of the character presence. . . . From this we can see that the character which appear in most episodes are Tyrion Lannister followed by Jon Snow, Sansa Stark, Daenerys Targaryen which makes perfect sense as these characters are main characters. Only a couple of characters are present only ones which clearly would indicate they had a small role in the Game Of Thrones plot. . Reviews and ratings . Lastly we will dive into the data from IMDB, where ratings and reviews are extracted. Here we will investigate how the rating distribution are in general, but also how it is distributed when taking the average rating pr. episode but also pr. season. . We will start out by looking at the average rating pr. season. From the figure below we can see that season 1 through season 7 have almost the same average rating, whereas season 8 clearly sticks out with a low score. Further it should be noticed that season 4 has the highest average rating of 9.31 which is quite high as the highest IMDB score are 10. Further it should be noticed that the season in general has a high average rating. . Season 8 having the lowest score does not come as a surprise as a lot of people were unhappy with the ending of the series, and a lot of people did feel that they just ended the series to quick. . . . Next we will look at the average rating pr. episode, to see if we could find any patterns. From the figure below we see approximately the same pattern as above, but we can now see that often the last 2 episodes in a season do achieve a higher average score compare to the middle episodes. Further it should be noticed that from the beginning of season 8 the episodes do keep getting lower average score, and the last episode in season 8 do achieve a quite low score of only 4. . . . We will dive into the demographics of the reviewers of Game Of Thrones, to see if a specific group of people watch the series, as this could help us understand any patterns and let us dive deeper into the case. . The figure below shows the distribution of number of votes across 4 age groups and gender. From this it can be seen that males do vote more the females, and also the largest age is age 30-44 whereas the smallest are aged under 18. . . . We could now look into how the average rating was distributed across gender and age. From the figure below it can be seen that males across all age groups give approximately the same high average rating, whereas females in the age group under 18, give the lowest average rating. It should be noted that this age groups also was the smallest, so this score can easily be affected by fewer people giving low average rating compared to the other groups. . . . We have now investigated the data used in this project. How the attributes of the characters are distributed, and dived a little further into the Game Of Thrones universe. We have investigated the dialogoue of the character from transcripts, and seen how many episodes and seasons these characters are present, and further what their average dialogoue length was. Lastly, we have dived into the basics of the voters for the series, how the demographics of the voters were and their average rating. .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/02_Basic_Statistics/",
          "relUrl": "/02_Basic_Statistics/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Game Of Thrones Network Analysis",
          "content": "How is the network generated? . This section will explain the methodology to produce the network of Game Of Thrones characters. This section will be divided into two parts, as the first part generates the network across all seasons and the second part will explain how the network is generated for each season. . Part 1 . The character network describing interactions between the characters across all seasons will be described in this part. To accomplish this, the Game Of Thrones Wiki-page is scraped. The site contains a section for each season describing all the characters present in each season. By iterating through all these pages on the wiki-page it was possible to extract characters by the use of regular expressions. Having the list of each characters present across all seasons, the next task would be to extract information about each character, and find the links between all characters. . In the next part we, again, utilize the Wiki-page of each character, again using the Game Of Thrones Wiki-page API to extract information of each character. Through the use of regular expressions it was possible to extract information about the allegiance, culture, religion, appearances and status of each character as this is listed in a fact-box on each character page (though some pages did not contain this information). Further, it was possible to find links between characters, as each page is linked to other characters that they somehow interact with. . As we wanted to put emphasis on how strong the link was between individual characters, as some might only be connected once and therefore not have a very large impact, we also saved the frequency of each unique connection between characters. . The network information had now been collected so each character have assigned the following attributes were available: . Allegiance | Culture | Religion | Status (dead, alive, uncertain) | Appearances | Which other characters it is linked to | Frequency of link | . We are now ready to generate the visualizations of the network, and this will be presented a little later in this section. . Part 2 . This part will explain how the networks were generated for each season, which would make it possible to investigate how the interactions between houses, cultures, characters etc. evovle through the series. Similar to the previous part we utilize the Wiki-page API to iterate through each season and extract the characters present in each season. . When extracting information for each character present in each season we did not extract the whole character page but only the part describing their presence in each season. Each character page contains a fact-box, a short introduction, and then a couple of sections describing their presence in each season (if present in multiple seasons). This could be utilized to extract interactions for each season. . We,again, include information about allegiance, culture and religion, which we extract from the network generated in the previous part. Again the frequency of each link is included. . Degree distribution . The network across all season contains 162 nodes ie. characters which is linked by 3085 edges. Having generated the network we are going to investigate some of the properties of the network. . We are starting out by examining the degree distribution of the network, as the network is a directed network this would include both the in degree and out degree distribution. The in-degree is the number of inward edges from a node to the given node and vice versa for the out-degree. The degree describes the number of edges, and can contain information describing the characters connectiveness in the network. . . . From the figure above it can be seen that the in-degree and out-degree distribution are very similar, and appears to come from the same distribution. It should be noted from both the histogram and the boxplot that the in-degree distribution has more extreme point but at the same time has a lower median. Further it can be seen that the out-degree (ie. outgoing links) appear to have a higher peak around 5-9 degree compared to in-degree. It should also be noticed that the network is very dense, and all characters have at least a couple of other characters they interact with. From the degree distribution we can find the top-5 most connected characters based on in- and out-degree which is presented in the tables below. . . . From the table above it appears that Jon Snow is the most connected character based on in-degree. Also it should be noted that all five characters are main characters in Game Of Thrones, and therefore it would make sense that they are well connected in the network. Further, all but Eddard Stark are characters that are appearing in most episodes (see Basic Statistics). . Eddark Stark dies quite early in the series, and it might be a surprise that he is one of the most connected characters, but as this is based on in-degree this could be due to many of the others characters pages references Eddard. This would make sense as his children probably talks about him/mentions him and therefore make him very connected compared to many other characters. . Based on out-degree again Jon Snow is the most connected character, but we see that Eddard Stark, Daeneras Targaryen and Cersei Lannister are replaced by Sansa Stark, Arya Stark and Jamie Lannister which also are very well connected characters and also appears as main characters in the series. . Network graph . The network generated are now going to be presented in the interactive figure below. The network contains multiple settings for the user to choose. Firstly, it is possible to choose whether the network should be based on all text based from the character pages ie. all season, or whether the user wants to inspect the network based on a separate season. Further, different overlays can be chosen based on the character attributes namely: religion, allegiance or culture. . When selecting a character one will be presented with an image of the character, and a short description of the character attributes, a link to the wiki-page and the most frequent used words by the character based on TF-IDF which will be explained in the Text Analysis section. . Attribute relationships . Next we are going to investigate how the attributes relate and interact with each other, also it could be that some attributes eg. religion is more well connected than others. We will start out by examining the allegiance attribute. . . . The figure above shows the top 10 most connected allegiances in Game Of Thrones, and it can clearly be seen that House Stark is the most connected allegiance, but a lot of the connectivity derives from the interaction with their own allegiance. Further, it can be seen that House Stark is well connected with House Lannister, Night&#39;s Watch and House Bolton. The connection with House Lannister and Night&#39;s Watch can easily be explained by eg. Ned Starks&#39; work as the Kings Hand but also Sansa Stark being married with Joffrey. Further, Jon Snow from the House Stark allegiance are becoming part of Night&#39;s Watch can explain this interaction. . Generally, it can also be seen that the allegiances interacts with it-self most, compared to interaction with other allegiances. . Next, we will look into how the religions interact, and from the figure below, it can clearly be seen that the two main religions are the religions that mainly interact with each other, which does not come as a surprise. . . . In the figure below, the connection between cultures are investigated. Here it can be seen that the Andals are the most connected culture, followed by Northmen. Furthermore, it can be seen that these two cultures interact alot. . It can be seen that the third most connected culture is Valyrians which does not mainly interact with themselves, but instead are most connected to Andals. . . . Network assortivity and centrality . In this section we are going to investigate some of the network properties of the character network across all seasons. In order to further investigate network properties we are going to compute assortivity and centrality of the network. . From the table below it can be seen that religion has the highest assortivity score, which would indicate that this attribute is the best to distinquish the characters from each other. It should be noted that none of the scores are very high indicating, that the characters are linked in a more complex pattern, or based on another attribute. . . . We are further going to investigate the closeness centrality of each node in the network, which measures the reciprocal sum of shortests paths from the given node to all other nodes. If a node therefore has a high closeness centrality score this means that the node is close to the rest and vice versa. This could give us an indication of well connected characters and further important characters. . In the table below the closeness centrality score is computed for all characters and sorted in descending order. As expected it can be seen that Jon Snow, Daenerys Targaryen and Tyrion Lannister are some of the characters close to the others. Further, Stannis Baratheon has a high centrality score, which would make sense as he is involved both as an heir to the throne, but also his involment with the red priest Mellisandre. . Further, Gregor Clegane, Eddard Stark, Bronn and the Night King has a high centrality score, which does not come as a surprise as these are key characters in the story, and interacts with many characters. . . . Most connected characters . In this section we are going to investigate which characters are the most connected characters in each season utilizing the networks for each season. This will be done based on in- and out-degree as in one of the earlier sections in this part of the website. . In the interactive figure below it is possible to investigate in-, out-degrees and closeness centrality score for each character in each season. This makes it possible to investigate how connected the characters are in each season, and this might give an indication of which characters that are most important in each season. . It is possible to search for specific characters, and sort the values. This will be done in both the table and the bar-plots below. Furthermore, attributes and characters can be deleted if needed. . From the table it can be seen that the most connected and maybe important characters in season 1 might be Eddard Stark and Robert Baratheon, which makes perfect sense as Eddard is present a lot in season 1, both in Winterfell but also when he becomes the Kings hand. Robert is also very central in season 1, as he rules as King and dies when hunting - killed by a pig, as he says it. . Season 2, here Joffrey Baratheon, the new king, rules, and Robb Stark goes to war as he wants revenge for his fathers execution in season 1. . Later, in season 7 Jon Snow and Daenerys Targaryen becomes key players as the winter is approaching and the focus moves from Kings Landing to the wall and the fight against the dead. . Subconclusion . In this section we have investigated the Game Of Thrones character network both across the full series but also in each season. We have found the most connected (and maybe most important) character across the full story but also in each season. This was done by investigating the properties of the network which both include the centralitiy and degree of each character. . Further, by investigating the character attributes we could dive into how these interacts with each other, and this did reveal patterns in which religions, allegiances and cultures that mainly interact. . Lastly, it was discovered through computation of assortivity of each attribute that these standing alone did not describe how the network was organized and that other methods would be needed. Later on we are going to try to detect communities in the network without using these attributes alone. .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/GameOfThronesNetwork/",
          "relUrl": "/GameOfThronesNetwork/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Text analysis",
          "content": "This section regarding text analysis is divided into two parts: namely wordclouds and sentiment analysis. Both the extracted wiki pages and the character dialogoues will be used and it will be investigated how wordclouds and sentiment analysis will differ based on the two different data sets. . Wordclouds . First, we will take a look at word clouds. As mentioned before, both the extracted wiki pages and the full series dialogoue will be investigated. We will start by generating wordclouds for characters of interest. Here, we have selected the characters: Jon Snow, Arya Stark, Bronn, Brienne of Tarth and Jaime Lannister. The first step in generating the wordclouds is to compute the term frequeny-inverse document frequency (TF-IDF) for our respective text corpus, i.e. the wiki pages and episode dialogoues. For further explanation of the TF-IDF and it&#39;s computation we refer to the Explainer Notebook. It should be mentioned that we have removed all characters&#39; names from the text corpus as these would not be very decriptive of the character in a wordcloud or during sentiment analysis. . Now, let&#39;s take a look at the generated wordclouds for the selected characters. . Wordclouds based on character wiki page &amp; dialogoue&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; When comparing the generated wordclouds for the respective data sets it should be noted, that the same words are, for the most part, not present for the respective characters. This is expected as one would imagine that the text from the characters wikipedia pages are more descriptive of the character and their place in the story whereas the wordcloud from the dialogoue is exactly that; their most descrriptive words according to TF-IDC used throughout the series. This would be interesting to compare with sentiment analysis which is the second part of this page. . Wordclouds based on selected houses . Next, we will generate wordclouds based on the characters allegiance. This will be done by pooling the dialogoue text of characters belonging to the same allegiance together and, again, compute the respective TF-IDF score in order to generate the wordclouds. For this, we have selected the houses: Stark, Lannister, Targaryen, Greyjoy and the independent group The Night&#39;s Watch. It would be interesting to see, if the houses mottos would appear in these word clouds. The respective house mottos are: . House Stark: Winter is coming House Lannister: Hear Me Roar! House Targaryen: Fire and Blood House Greyjoy: We Do Not Sow . As the Night&#39;s Watch is not a House but rather a brotherhood sworn to protect The Wall, they do not have a motto. . When looking at the wordclouds above and the respective house mottos, only the Lannisters&#39; Hear (big, middle) are present. All the wordclouds are, however, very descriptive of the respective houses. For instance for the Night&#39;s Watch, a military order sworn to protect The Wall, words like protect, wildling and swear are present. The same can be said for House Targaryan, where the main Targaryan character, Daenerys, is married to a dothraki warlord and later in the show, is a leader of dothraki people herself. . Wordclouds based on seasons . We will now generate wordclouds based on the wiki pages&#39; season sections. It would be interesting to see how these wordclouds change as the story unfolds. It would also be intersting to investigate whether the overall theme of the series changes during the series course and if this can be seen in the wordclouds. . Taking example in the wordclouds generated for season 1 &amp; 8, the emphasized words seem very descriptive of their respective seasons. Starting with season 1: . execute, behead : One of the main acts of season 1, is the execution of Lord Eddard Stark, the head of House Stark. He is, by the unexpected command of the king Joffrey Baratheon, beheaded in the middle of King&#39;s Landing. | Khal, bloodrider : Another of the main story arcs, is the story of Daenarys Targaryan which takes place in a foreign land. In season 1, Daenarys is married of to a powerful Khal, Khal Drogo, in a trade by Daenarys brother. A Khal has three bloodriders who are to live and die by the life of their Khal. The words Khal and bloodrider being so prominent makes sense, as they are key roles in Daenarys&#39; story arc. | . Comparing the wordclouds of season 1 and season 8, it appears season 8 has different key words. For season 8: . celebrate : The word celebrate stands in stark constrast to the prominent words suffer from season 1. This could be due to season 8 being the series final season and it&#39;s characters are therefore celebrating the story ending on a happy note (for some of the characters :wink: ) | reunite : The story culminates in the final season, many characters who have been seperated throughout the show are finally reunited in the final season of the show, hence emphasis on the word reunite makes sense. | . It should also be noted that the word destroy is present in the majority of the wordclouds, only being omitted in the wordclouds for season 1 and 3. . Sentiment of characters . In this second part of text analysis, we will do a sentiment analysis of the characters, again, based on both their wiki-pages and their dialogoue in the series. As we saw in the wordclouds of the selected characters, there was quite a difference in the wordclouds based on the respective wiki-pages and character dialogoue. It would be interesting to look at, if this also results in a different sentiment level of the character. Additionally, we will also do a sentiment analysis of the different seasons of the series. Perhaps it can be determined if any of the seasons were significantly different on a sentiment based level. . For the sentiment analysis, we will apply both the dictionary based method of LabMT and the rule- and dictionary-based method of VADER. For further explanation of how these sentiment scores are computed and the difference between the two methods, we again refer to the Explainer Notebook. It should be noted that the score of the two methods differ, as the LabMT score sentiment on a scale from [1:9], while VADER scores on the range [-1:1]. For LabMT, a score of 5 is considered neutral while a score within the range [-0.05:0.05] is considered neutral for VADER. . Sentiment analysis of character dialogoue . In this subsection we are going to investigate the sentiment of characters based on their dialogoue which is based on transcripts. This is based on all dialogoue across all seasons as this is expected to give a better overview of each character sentiments. . The figure below presents the sentiment of the 10 happiest and 10 sadest characters. To the left the sentiment are based on LabMT whereas the figure to the right is based on VADER. . It should be noted that the two methods does not completely agree, but some characters are present in both results such as: Daisy, Pyat Pree, Olyvar and Matthos Seaworth are in top 10 of the happiest character in both results. Also some characters are present in both lists presenting the sadest characters such as Gregor Clegane. . The happiest characters appear to be quite happy based on the VADER and LabMT score as the score only goes to 1 for VADER and 9 for LabMT and the same for saddest characters. . Sentiment analysis on character wiki pages . This subsection is going to investigate the sentiment of each character based on their character wiki page. We are further going to compare this with the sentiment of characters based on their dialogoue. . From the figure below it can be seen that the two methods, again, do not completely agree on the result but both methods yield approximately the same result. Again the figure displays the 10 happiest and sadest characters based on LabMT and VADER. . At a first glance, it is noticed that the VADER score are lower for the happiest characters than in the previous part whereas the sadest achieve almost the same score. The LabMT results are quite similar in sentiment levels. Again many characters are found in both results such as Septa, Moro, Orell and Polliver. . When comparing with the result based on the character dialogoue not many characters are found in all four results. This could indicate that the wiki-pages and dialogoue does not contain the same information, or that the chosen words on the wiki-pages do not necessarily imply information about the characters sentiment. . It would be expected that the dialogoue contains greater variety of words that can explain the character mood, whereas the wiki-pages would contain words that describe the character and his/hers actions. We also notice that the variation in VADER sentiment scores are far greater when using the dialogoue compared with the wiki-page which could be an indication that our hypothesis are true. . Sentiment analysis on the series&#39; seasons . As a last element in our sentiment analysis we are going to dive into the sentiment of each season. This could help us investigate whether the general mode changes in each season and when combining this with the wordclouds of each season, indicate whether the theme of the series changes as it progressses. . The figure below shows the sentiment of each season based on LabMT and VADER methods. When looking at the LabMT it can be seen that all season are approximately neutral, whereas the VADER scores are just to the sad side of the spectrum. Further, it is noticed that season 4 are the sadest whereas season 6 are the &quot;happiest&quot; when comparing them. . In season 4 a lot of the semi-main characters die such as Prince Oberyn, Joffrey Baratheon, Shay, Tywin Lannister and the Mountain (Gregor Clegane) are transformed into the Monster version of himself. Which could explain why this season is saddest according to the sentiment analysis. . Dispersion plot . As a last element in our text analysis we are going to investigate some words and how they are used across different seasons, this could again help us understand how the theme of the series evolves. We are going to investigate the use of the selected words by the use of a lexical dispersion plot. . Looking at the lexical dispersion plot above, the first word we chose was winter. This is due to the famous Stark house words being &quot;Winter is coming&quot; and we wanted to investigate how much this phrase was actually used. It appears winter is most used in the beginning and the end of the show. Only sorting for the word winter has the caveat though, that other common phrases such as the long winter are also represented here. . Another interesting comparison is the words dragon and wolf. Both the Targaryens and Starks are refered to as dragons and wolves respectivly but the Stark children also raise their own dire wolf throughout the show. The same can be said for Daenarys whose dragons are born in the end of season 1 and raised througout the show. In the beginning of the show, the wolves are more commonly mentioned compared to the ending where they are barely mentioned. The opposite holds true for dragon which is less mentioned in the beginning but mentioned more and more as the story unfolds. . It can also be seen that the word wedding is mentioned most during season 3 and season 4. This holds true to the story as both Robb Stark, Joffrey Baratheon and Sansa Stark are all married during these seasons. . Subconclusion . During this section, we have found the most important words on a character, allegiance and seasonal level. The found words and their importance were represented with wordclouds and it was found that the chosen words represented the character, allegiance and season well. A comparison of the different data sources of wiki pages and show dialogoue has also been made, where it was found that the choice of words in these data sources are significantly different. This also makes sense, as it would be expected that the wikipedia page is more descriptive of the story,its characters and the setting whereas the dialogue would be expected to be a more crude choice of words. . Using sentiment analysis, the happiest and saddest characters of the show were found. Both according to the characters dialogoue but also their wikipedia pages. It was also found that the sentiment of wikipedia pages are different than the sentiment of the dialogoues. . Finally, a lexical dispersion plot was computed in order to visualise how some of the common words of the Game of Thrones world were used throughout the show. . &lt;/div&gt; .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/textanalysis/",
          "relUrl": "/textanalysis/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Community Analysis",
          "content": "This section contains three parts: the first part we will explain how we generate the communities and investigate the basics of the communities. Next we will investigate the language used by these communities, namely find the words with the highest TF-IDF. Lastly, we are going to investigate the sentiment of these communities. . Community detection . In this section, we will find communities in the Game of Thrones network. These communities are identified using the Louvain Algorithm. Further explanation of the Lovain Algorithm can be found in the Explainer Notebook. In total, six communities are identified with a modularity value of 0.31. Below, some exploratory analysis can be seen of these 6 communities using the character attributes religion, culture and allegiance. . From the figure below the size of each community is presented. From this it is clear that Community 1 is the largest community. Whereas Community 0 is the smallest. . . . Next, we will investigate what religions, cultures and allegiances each community contains, as this can help us understand what the Louvain algorithm have chosen in order to split the characters into the different communities. We will start out by looking into religion. . . . From the above we can notice that Community 0 contains equal parts of characters being part of Faith of the Seven religion and Drowned God, whereas Community 1 mainly contain characters being part of Faith of the Seven. . Community 0 could therefore be expected to contain people from the Iron Island and as well from Kings Landing, whereas Community 1 contains primarily people from Faith of the Seven, and 1 from the Old Gods of the Forest, this could maybe be people being from House Lannister and Sansa Stark as these characters are very well connected in large parts of the story. . Next, in Community 2 the religions are a mix of the Great Stallion, the Old Gods of the Forest, the Faith of the Seven and Ghiscari. This could be due to the characters surrounding Daenarys Targaryen, as her group are a rather mixed group. . The next three communities are also rather mixed, and could represent multiple groups of characters thus being rather complicated to decrypt. . . . From the figure above it can be seen that the communities are very mixed when it comes to allegiance, which would be expected as the Game Of Thrones universe contains a lot of different allegiances, and these are mixing up together. Eg. the people from the North are part of many different allegiances, but are grouping together when needed for eg. war. . One could see that the largest allegiance in community 1 are House Lannister, but also contains Petyr Baelish, Kingsguard, Sparrows, House Clegane etc. This community appears to describe the important people of Kings Landing. . Community 2 is described by House Targaryen, Second Sons, Unsullied, Drogo and could describe the group sorounding Daenarys Targaryen. . Community 3 is described by House Umber, Free Folk, Night&#39;s Watch and White Walkers, maybe this community describes the people interacting at the Wall and the land around it. This could explain this group of allegiances. . . . Lastly, we are looking into what culture each community contains. Here it can be seen that the Andals are the largest in Community 1, 3, and 5. This would be expected as this is the largeset cultural group in Game Of Thrones. . Community 2 contains large group of Dothraki people, and this again underlines that community 2 could be the group of characters surrounding Daenarys. . Similarly, we see that community 3 contains Free Folk, Northmen, White Walkers, Children of the Forest, which could indicate the people at the Wall and beyond. . Community character exploration . In this section we will visualize each of the communities in the same way that it was done in Game Of Thrones Network Analysis in an attempt to understand how the communities have been detected. From analysis in the above part we did encounter some patterns that could explain why the communities was created as they are. We will now dig into what characters are present and how they are connected. . We encourage you to use the functionalities of this app to investigate how the characters interact in each community in which attributes that are present. . TF-IDF and wordclouds . As in the Text Analysis section, we will compute TF-IDF for each community, which can help us investigate the properties of the different communities. . As each community consist of different characters, and we know which words each character are described by from its character page on the Wiki-page. From this we can thereby first compute the TF-for each community, and afterwards compute IDF for all the words used across the communities. From these computations we can now compute the TF-IDF. To see the full computations of the TF-IDF these can be found in the Explainer Notebook. . The computed TF-IDF are used to size the words when visualizing the words describing each community. The results of the analysis are presented in the figure below. . We notice that the words are very descriptive. If we use community 2 as an example we can see that these words match very well with the expectation that this group describes the people around Daenerys Targaryen. This includes khal, khalasar, invade, reclaim, conquer as she starts out with being wife of Khal Drogo and being part of his khalasar. Later she invades multiple cities and conquer a large part of the Game Of Thrones universe. Further she tries to reclaim the Iron Throne. . The same pattern of words describing the community can be seen in the other wordclouds. . Sentiment analysis . This part will dive into the sentiment of each community, this will be based on character dialogoues instead of the wiki-pages as used in the previous part. This is done as it is expected that the dialogoue of each character better resembles the character&#39;s mood rather than a wiki-page describing the character. . As in the Text Analysis section this will be based on LabMT and VADER sentiment methods. . . . The figure above displays the computed sentiment values for each community based on LabMT and VADER sentiment methods, further the standard deviation are displayed as an error-bar, which can indicate whether a community has a large variation in sentiment values. It can be seen that based on LabMT all communities have almost the same sentiment level and rather small variation, though community 1 has the largest variation, which also can be seen from the VADER sentiments. . The VADER sentiment appear to have a little larger variation between the communities and as well within each community. . The VADER score ranges from -4 to +4 and a score of zero indicates that the sentiment is neutral. From this it can be seen that community 2 and three are slightly positive, whereas community 1 are more positive but not happy. Further community 5 appears to be the most sad community. When looking at the LabMT score it ranges from 1.3 to 8.5 where a score of 5 is neutral. Again all the communities are close to being neutral where community 1 is the most happy and community 3 and 5 are the saddest which corresponds with the results from the VADER sentiment scores. . From the sentiment analysis of each community we find that the most happy community is Community 1 and 4 whereas the sadest are Community 5 and 3. . Subconclusion . Through this section we have investigated whether the characters are grouped based on an underlying pattern which is not only based on eg. allegiance, culture or religion. Using the Louvain algorithm it was possible to detect six communities, which appears to be described by which characters interact during the series . . It was possible to detect the main character(s) of each community by the use of network analysis. We further investigated these communities through the use of text- and sentiment analysis, and found the words used by each community were well connected with the characters present in this community. The words used by each community helped reveal the underlying structure behind each community. Lastly, the sentiment analysis revealed the mood of each community. .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/CommunityAnalysis/",
          "relUrl": "/CommunityAnalysis/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Explainer Notebook",
          "content": ". 1. Motivation . Game Of Thrones is a very popular series, and achieved the status of being the most watched series to date. The series consist of multiple characters, allegiances, religions, locations and at the same time all these stories play out concurrently. Which makes the information and storytelling at some point quite overwhelming - do all of these characters have a pattern in how they act? Are there any pattern in how their story play out? Could we forecast these events? . We wish to dig into these massive amounts of informations of each character to investigate if we could unveil any underlying pattern which could help all the Game Of Thrones fans understand the story even better. This would include investigation of network patterns, but also if we could find the most important character in each season, and overall in the series. Also to see if we can find any underying communities and thereby see if we can decoded the underlying pattern. . 1.1 What is your dataset? . Our main dataset is from The Game Of Thrones wiki-page, where we have extracted the character pages for all characters present in the Game Of Thrones series. This dataset is restricted to only contain the characters that are stated on the Wiki-page, and when this is compared with the number of characters from the book, the data amount in number of characters is smaller in comparison. . The data for each character are both extracted as a total amount of data across all seasons ie. all the data from each character page, but also based on each season. From the character page some basic attributes were extracted: allegiance, religion, culture, appearances, status and what other characters the given character were linked to, and how often these were linked. The thumbnail image was also extracted for later visualization, and lastly the text was cleaned for the purpose of text analysis later on. . As the wiki-pages are written by other people this could effect the sentiment of the character pages, therefore it was also decided to include an extra data source namely the character dialogoues. A data-source containing clean transcripts from all the seasons and episodes were used, so no data cleaning was needed here besides excluding all the dialogoue of characters that was not found through the wiki-page data. . Lastly, ratings and reviews from IMDB was used for each episode, as it was wished to compare this with the dialogoue. This was found by the use of a python package, which extracted data from IMDB. We did further investigate the possibility to scrape the IMDB webpage for reviews, but later it was found that this was not allowed and therefore we did not further pursue this opportunity. . . 1.2 Why did you choose this/these particular dataset(s)? . The dataset from the Game of Thrones wikipage, was used as this could both deliver information about each character ie. some of their general attributes but also text relating to interactions of the characters but also words describing each character. This could help us understand the interactions between all characters, and hopefully help us understand some of the underlying patterns in the series. . We could investigate the network properties of the series both on an overall level across all seasons but also in each season. Through this we could make an analysis of each season, and investigate how this developed through each season. By investigation of words describing each character, and on the basis of their attributes eg. allegiance, we would perform text and sentiment analysis to see if we could understand the grouping. . As the sentiment of the character pages could be biased by the writer of the page we wanted to find the root source namely the transcripts from the series, and therefore we utilized the dialogoue of all the characters as an second data source. This was done as we thought this could add extra valuable information to understand how the characters were feeling, and furthermore how they were grouped. . Lastly, we added the ratings from IMDB to see if the reviewers was affected by the general mood in the series. . 1.3 What was your goal for the end user&#39;s experience? . The goal of the website is to present nice interactive visualizations that engage the user to explore the data and the Game Of Thrones series. Further, the site should be easy to grasp and understandable also for people without the theoretical knowledge learned in the course. The website contains results presented in different visualizations and tables that the user can dive into, and should be tempted to dive deeper into the analysis and the results. The site should be engaging, and it should not be necessary to visit the site for hours in order to get any insights, though this should be an opportunity. . The website should let the end user explore the complex character interactions from the Game Of Thrones series, and let the user understand some of these patterns, which is explored both through network analysis, text analysis and sentiment analysis. The user should understand which characters are the most important characters in the series but also in each season, and further explore these characters properties. This could include words they use, how the characters feel and who they interact with. The user should feel the urge to dive deeper into this, and should feel confident in what the data contains and can be used for. . 2. Data and basic statistics . 2.1 Data extraction, cleaning and pre-processing . import networkx as nx import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import plotly.graph_objects as go import json import requests import plotly.graph_objects as go import urllib import nltk import re from bs4 import BeautifulSoup from nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizer from plotly.subplots import make_subplots import os from nltk.corpus import stopwords from nltk import FreqDist from nltk.tokenize import wordpunct_tokenize import string import itertools from imdb import IMDb nltk.download(&quot;stopwords&quot;) nltk.download(&#39;punkt&#39;) nltk.download(&#39;wordnet&#39;) . [nltk_data] Downloading package stopwords to [nltk_data] C: Users Mikkel AppData Roaming nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package punkt to [nltk_data] C: Users Mikkel AppData Roaming nltk_data... [nltk_data] Package punkt is already up-to-date! [nltk_data] Downloading package wordnet to [nltk_data] C: Users Mikkel AppData Roaming nltk_data... [nltk_data] Package wordnet is already up-to-date! . True . 2.1.1 The Game of Thrones Wiki-page &lt;/h4&gt;&lt;/p&gt; The main data set of this project has been extracted from the Game of Thrones fan Wikipedia. In order to generate a Game of Thrones network, it is first nescessary to compile a list of characters to build the network on. This was done by, for each season, using the wiki API to extract the cast of the series as seen below. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def api_call(t): # Basic function to call the Wiki-pedia API baseurl = &quot;https://gameofthrones.fandom.com/api.php?&quot; action = &quot;action=query&quot; title = &quot;titles=&quot;+t.replace(&#39;&#39;, &#39;e&#39;) content = &quot;prop=revisions&amp;rvprop=content&amp;rvslots=*&quot; dataformat =&quot;format=json&quot; query = &quot;{}{}&amp;{}&amp;{}&amp;{}&quot;.format(baseurl, action, content,title, dataformat) wikiresponse = urllib.request.urlopen(query) wikidata = wikiresponse.read() wikitext = wikidata.decode(&#39;utf-8&#39;) j = json.loads(wikitext) return j def get_vals(nested, key): # Extracts data from a nest dictionary. Taken from: ## https://stackoverflow.com/questions/67747851/python-extracting-data-from-nested-dictionary result = [] if isinstance(nested, list) and nested != []: #non-empty list for lis in nested: result.extend(get_vals(lis, key)) elif isinstance(nested, dict) and nested != {}: #non-empty dict for val in nested.values(): if isinstance(val, (list, dict)): #(list or dict) in dict result.extend(get_vals(val, key)) if key in nested.keys(): #key found in dict result.append(nested[key]) return result # Base url for the seasons base_cat = &quot;Game_of_Thrones_Season_&quot; names = [] pattern1 = &quot; [ [(.*?) ] ] as [ [(.*?) ] ].*&quot; for i in range(1,9): tmp = api_call(base_cat + str(i)) txt = get_vals(tmp, &#39;*&#39;)[0] names.extend(re.findall(pattern1, txt)) characters = {char: {&quot;actor&quot;: actor, &quot;link&quot;: char.split(&quot;|&quot;)[0].replace(&quot; &quot;, &quot;_&quot;) if len(char.split(&quot;|&quot;)) &gt; 0 else char.replace(&quot; &quot;, &quot;_&quot;)} for actor, char in names} . Using the method above, we generate a dictionary with the game of thrones characters as keys and the actor of the character and the wiki-link as values. In order to extract the character and actor names from the returned txt, we use the regular expression defined as pattern1. Looking at the website, it was noticed that the actor and his played role followed the simple syntax of: &quot;[[Actor]] as [[Character]]&quot;. So, the regex expression simply looks explicitly for two sets of [[]] seperated by &quot;as&quot; and extracts all characters enclosed by these brackets. . As we now have a compiled list of all characters present in the Game of Thrones series, we now want to extract the character wiki-pages. This is done similarly as above, simply replacing the season URL with the character URL. . links = get_vals(characters,&#39;link&#39;) for link in links: j = api_call(link) if j[&#39;batchcomplete&#39;] != &#39;&#39;: print(&#39;error:&#39;, link) else: txt = get_vals(j, &#39;*&#39;)[0] if(&quot;REDIRECT&quot; in txt): tmp = re.findall((&quot; [ [(.*) ] ]&quot;),txt) j = api_call(tmp[0].replace(&quot; &quot;, &quot;_&quot;)) txt = get_vals(j, &#39;*&#39;)[0] with open(&#39;/work/got/&#39;+link+&quot;.txt&quot;, &quot;w&quot;) as text_file: text_file.write(txt) text_file.close() . It should also be noted, that if the returned text contains &quot;REDIRECT&quot;, we perform a new API call to the link specified by the redirect term. Finally, all the returned character pages are saved as .txt files. . Creating the Game of Thrones Network . We can now generate the overall Game of Thrones network containing all the listed characters as network nodes. This is done by iterating through all the extracted character .txt files. The whole code chunk can be seen below but is easier understood if seperated into specific parts and purposes. . Extracting character links: In order to compute edges for our Game of Thrones network, we iterate through all the generated .txt files and look for any links to other game of thrones characters. This is done by using the following: . pattern = &quot; [ [(.*?) ] ]&quot; links = re.findall(pattern, node_description) links, counts = np.unique([link.replace(&quot; &quot;, &quot;_&quot;) for link in links if link.replace(&quot; &quot;, &quot;_&quot;) in char_list], return_counts=True) final_links = [] for i, link in enumerate(links): final_links.append((name,link,counts[i])) G.add_weighted_edges_from(final_links) . All links on the Game of Thrones wiki-page are enclosed by double brackets [[link]]. For each saved .txt file, we search for all links in the given file and look up whether this link is a characters in Game of Thrones. Additionally, we also count how many times a character is linked. This count will constitute the given weight of an edge. Finally, all the found links and their respective weight are added to the given node. This is then done for all nodes in the network. . Extracting character attributes: On the wiki-page, most of the Game of Thrones characters have a fact box containing generic information such as: allegiance, religion, culture, status, amount of episodes and more. We extract information regarding culture, religion and allegiance using the following regex expression: . culture_pattern = &#39;((?&lt;=Culture[ s = s]{1})|(?&lt;=Culture[ = s]{2})|(?&lt;=Culture[ s = s]{3})) [ [(.*?) ] ]&#39; religion_pattern = &#39;((?&lt;=Religion[ s = s]{1})|(?&lt;=Religion[ = s]{2})|(?&lt;=Religion[ s = s]{3})) [ [(.*?) ] ]&#39; allegiance_pattern = &#39;((?&lt;=Allegiance[ s = s]{1})|(?&lt;=Allegiance[ = s]{2})|(?&lt;=Allegiance[ s = s]{3})) [ [(.*?) ] ]&#39; . The above regex expressions contains three cases of different positive lookbehinds. The positive lookbehind is expressed using ?&lt;= and simply looks for the given attribute, i.e. Culture for instance. The wiki-page uses three different formats in their textboxes: &quot;Culture = &quot;, &quot;Culture= &quot; or &quot;Culture =&quot; which is the reasoning for the use of three different positive lookbehind cases. If any of these positive lookbehinds match we extract the given information enclosed in [[]]. To sum it up, we only extract information enclosed in [[]] if these brackets are preceded by a match of one of the three different cases of positive lookbehind. We extract information regarding Status and number of Apperances in a similair way. . Minor pre-processing of extracted attributes There is some inconsistencies regarding naming on the wiki-pages. For instance, both Andal and Andals is used regarding characters culture. This is handled simply by the use of if-statements as seen below. Other inconsistencies are handled similarly. . if culture == &#39;Andal&#39;: culture = &#39;Andals&#39; . Extraction of thumbnail and formatting of text for network application In order to extract links to thumbnail images for the network app seen on the webpage, we made use of the python library BeautifulSoup. This is done by parsing the html code of the given character site, and looking for the keyword img of class pi-image-thumbnail and extracting the src link. This extracted image link is then added to the network as an attribute for the use of the network app. . Similarly, the extracted character name, attributes, most used words and a link to the characters&#39; wiki-page is also added to the network as a string under the attribute text. This is also for the use of the network app. The network app will be further explained in Section 3.1 and how the most used words are found is explained in Section 3.2. . Below is the used code for creating the compiled Game of Thrones Network and the helper functions to extract image thumbnails for the network app. . def getdata(url): r = requests.get(url) return r.text def get_img(name): htmldata = getdata(&quot;https://gameofthrones.fandom.com/wiki/&quot;+name) soup = BeautifulSoup(htmldata, &#39;html.parser&#39;) image = soup.find(&#39;img&#39;, attrs={&quot;class&quot;:&quot;pi-image-thumbnail&quot;}) return image[&#39;src&#39;] . char_list_all = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] char_names = [] char_names_cap = [] for char in char_list_all: char_names_cap.append(char) char = char.lower() char_names.append(char.replace(&quot;_&quot;,&quot; &quot;)) char_names.extend(char.split(&#39;_&#39;)) stop_words = set(stopwords.words(&#39;english&#39;) + list(string.punctuation) + list(char_names)) def clean_text(txt, charlist): txt = txt.lower() word_tokens = wordpunct_tokenize(txt) filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] wnlet = WordNetLemmatizer() words = [wnlet.lemmatize(w) for w in filtered_sentence] return words def tf_idf_func(): char_pages = {} characters = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] for char in characters: name = char.replace(&#39;_&#39;, &#39; &#39;) with open(&#39;../data/got_cleaned/&#39;+char+&quot;.txt&quot;, &quot;r&quot;) as text_file: txt = text_file.readlines() char_pages[name] = txt char_list = [char.split(&#39;.txt&#39;)[0] for char in characters] #Create space for dict: tc_dict_char = {} l_dict_char = {} wnlet = WordNetLemmatizer() #for com in top5_com: for char in char_list: try: words = char_pages[char.replace(&quot;_&quot;, &quot; &quot;)] except: continue word_tokens = wordpunct_tokenize(&quot; &quot;.join(words)) filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] words = [wnlet.lemmatize(w) for w in filtered_sentence] l_dict_char[char] = len(words) #Compute TC for the community words: tc_dict_char[char] = FreqDist(words) # Find all unique words unique_words = set(list(itertools.chain(*tc_dict_char.values()))) # Calculate idf idf = {word: #Find in how many documents that each word is present: np.log(len(tc_dict_char) / sum([tf[word] &gt; 0 for k, tf in tc_dict_char.items()])) #Iterate through unique words: for word in unique_words } #Create dict for tf-idf values: tf_idf_char_page = {} #Iterate through communities: for char in tc_dict_char.keys(): tf_idf = {} #Iterate through each word in each community: for word, tf_value in tc_dict_char[char].items(): #Extract IDF idf_value = idf[word] #Compute TF-IDF tf_idf[word] = idf_value*tf_value/l_dict_char[char] tf_idf_char_page[char] = tf_idf return tf_idf_char_page . files = os.listdir(&quot;../data/got/&quot;) char_list = [file.split(&#39;.txt&#39;)[0] for file in files] # List of all characters G = nx.DiGraph() #Create empty Directed Graph Object for file in files: # Iterating through all the saved .txt files for characters tf_idf_char = tf_idf_func() # For computation of most used words with open(&quot;../data/got/&quot; + file) as f: node_description = f.read() # read in current .txt file name = file.split(&quot;.txt&quot;)[0] # name of current character if name == &quot;Royal_Steward&quot;: #Special case continue pattern = &quot; [ [(.*?) ] ]&quot; # Pattern for finding links to other characters in the text file links = re.findall(pattern, node_description) # Extracting only links to other characters and count of how many times a character is linked. links, counts = np.unique([link.replace(&quot; &quot;, &quot;_&quot;) for link in links if link.replace(&quot; &quot;, &quot;_&quot;) in char_list], return_counts=True) # Regex expression for extracting character attributes culture_pattern = &#39;((?&lt;=Culture[ s = s]{1})|(?&lt;=Culture[ = s]{2})|(?&lt;=Culture[ s = s]{3})) [ [(.*?) ] ]&#39; religion_pattern = &#39;((?&lt;=Religion[ s = s]{1})|(?&lt;=Religion[ = s]{2})|(?&lt;=Religion[ s = s]{3})) [ [(.*?) ] ]&#39; allegiance_pattern = &#39;((?&lt;=Allegiance[ s = s]{1})|(?&lt;=Allegiance[ = s]{2})|(?&lt;=Allegiance[ s = s]{3})) [ [(.*?) ] ]&#39; status = re.findall(&quot; |.*Status.* [ [.* |(.*?) ] ]&quot;,node_description)[0] if len(re.findall(&quot; |.*Status.* [ [.* |(.*?) ] ]&quot;,node_description))&gt;0 else &quot;&quot; appearances = re.findall(&quot; |.*Appearances.* ( d+)&quot;, node_description)[0] if len(re.findall(&quot; |.*Appearances.* ( d+)&quot;, node_description))&gt;0 else &quot;&quot; allegiance = re.findall(allegiance_pattern, node_description)[0] if len(re.findall(allegiance_pattern, node_description))&gt;0 else &quot;&quot; allegiance = allegiance[1] if len(allegiance)&gt;1 else allegiance allegiance = allegiance.split(&#39;|&#39;)[1] if len(allegiance.split(&#39;|&#39;))&gt;1 else allegiance culture = re.findall(culture_pattern, node_description)[0] if len(re.findall(culture_pattern, node_description)) &gt; 0 else &quot;&quot; culture = culture[1] if len(culture)&gt;1 else culture culture = culture.split(&#39;|&#39;)[1] if len(culture.split(&#39;|&#39;))&gt;1 else culture religion = re.findall(religion_pattern, node_description)[0] if len(re.findall(religion_pattern, node_description)) &gt;0 else &quot;&quot; religion = religion[1] if len(religion)&gt;1 else religion religion = religion.split(&#39;|&#39;)[1] if len(religion.split(&#39;|&#39;))&gt;1 else religion #Special cases if name == &#39;Tommen_Baratheon&#39;: allegiance = &#39;House Baratheon of King &#39;s Landing&#39; if culture == &#39;Andal&#39;: culture = &#39;Andals&#39; if culture == &#39;Valyrian&#39;: culture = &#39;Valyrians&#39; if allegiance == &#39;King of the Andals and the First Men&#39;: allegiance = &#39;King of the Andals, the Rhoynar, and the First Men&#39; if religion == &#39;&#39;: religion = &quot;No known religion&quot; # Text attribute for network app name_for_text = name.replace(&#39;_&#39;, &#39; &#39;).capitalize() if len(name_for_text.split(&#39; &#39;))&gt; 1: name_for_text = name_for_text.split(&#39; &#39;)[0].capitalize() + &#39; &#39; + name_for_text.split(&#39; &#39;)[1].capitalize() text = &quot;The selected character: &quot; + name_for_text + &quot; has the following attributes: &quot;+ &quot; n - Allegiance: &quot; + allegiance + &quot; n - Religion: &quot; + religion + &quot; n - Culture: &quot;+culture + &quot; nMost used words by &quot; + name_for_text + &quot; are: &quot; + &quot;, &quot;.join([word[0] for word in sorted(tf_idf_char[name].items(), key=lambda value: value[1], reverse = True)[:5]]) + &quot; nLink to character page are: https://gameofthrones.fandom.com/wiki/&quot;+ name.replace(&#39; &#39;, &#39;_&#39;) # Thumbnail image for network app try: thumbnail = get_img(name) except: site = &#39;Iron_Throne&#39; thumbnail = get_img(site) # Add given character as a network node with the found attributes G.add_node(name, **{&quot;status&quot;: status, &quot;appearances&quot;: appearances, &quot;culture&quot;:culture, &#39;allegiance&#39;: allegiance,&quot;religion&quot;: religion, &quot;text&quot; : text, &quot;thumbnail&quot;: thumbnail}) # Compiling the found links into network edges with weights = counts of links final_links = [] for i, link in enumerate(links): final_links.append((name,link,counts[i])) G.add_weighted_edges_from(final_links) . Creating the season Networks . In order to create networks only for a given season, we use a similair approach as the one to create the overall network. On most character pages on the Game Of Thrones wiki, there is subsections stating their story progression for each season. This information is used in order to create networks for each season. . We start out by creating a nested dictionary that for each season holds a dictionary with characters, respective actor and wiki-page links. We extract characters exactly as when creating the overall network with the only difference being that this is done for each season. . The code for this can be seen below. . names = {} pattern1 = &quot; [ [(.*?) ] ] as [ [(.*?) ] ].*&quot; for i in range(1,9): tmp = api_call(base_cat + str(i)) txt = get_vals(tmp, &#39;*&#39;)[0] tmp = re.findall(pattern1, txt) names[i] = {char: {&quot;actor&quot;: actor, &quot;link&quot;: char.split(&quot;|&quot;)[0].replace(&quot; &quot;, &quot;_&quot;) if len(char.split(&quot;|&quot;)) &gt; 0 else char.replace(&quot; &quot;, &quot;_&quot;)} for actor, char in tmp} . We now need to extract the text for the characters. This is done by iterating through the respective seasons, utilizing the just before created dictionary containing all characters present for a given season. The character links are extracted from the dictionary and using the wiki-api the character webpage is extracted as well. The relevant text for a given season is extracted by splitting the text as seen below. Again, the subsections of seasons are not consistently using the same format hence the many different if-statements. . The extracted text snippet for each season for a given character is then saved as a .txt file. . for i in range(1,9): links = get_vals(names[i],&#39;link&#39;) for link in links: j = api_call(link) if j[&#39;batchcomplete&#39;] != &#39;&#39;: print(&#39;error:&#39;, char) else: txt = get_vals(j, &#39;*&#39;)[0] if(&quot;REDIRECT&quot; in txt): tmp = re.findall((&quot; [ [(.*) ] ]&quot;),txt) j = api_call(tmp[0].replace(&quot; &quot;, &quot;_&quot;)) txt = get_vals(j, &#39;*&#39;)[0] if len(txt.split(&quot;===[[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]]===&quot;))&gt;1: txt = txt.split(&quot;===[[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]]===&quot;)[1] elif len(txt.split(&quot;===[[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;))&gt;1: txt = txt.split(&quot;===[[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;)[1] elif len(txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]]===&quot;))&gt;1: txt = txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]]===&quot;)[1] elif len(txt.split(&quot;===[[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;))&gt;1: txt = txt.split(&quot;===[[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;)[1] elif len(txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]]===&quot;))&gt;1: txt = txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]]===&quot;)[1] elif len(txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;))&gt;1: txt = txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;)[1] elif len(txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;))&gt;1: txt = txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;)[1] elif len(txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;))&gt;1: txt = txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;)[1] elif len(txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;))&gt;1: txt = txt.split(&quot;=== [[Game of Thrones Season &quot;+str(i)+ &quot;|Season &quot;+str(i)+&quot;]] ===&quot;)[1] else: continue txt = txt.split(&quot;==&quot;)[0] with open(&#39;../data/got2/s&#39;+str(i)+&#39;/&#39;+link+&quot;.txt&quot;, &quot;w&quot;) as text_file: text_file.write(txt) text_file.close() . We now know all characters for a given season and have extracted the relevant part of their character page. Hence, we can, again, create the networks using the almost the same approach as when creating the overall Game of Thrones network but using the seasonal character texts instead. Instead of extracting character attributes again using regex expression we utilize that this information is already stored in the overall Game of Thrones network instead. . One minor tweak for how we create these seasonal networks is, that a lot of the character references in the seasonal text files are not referenced by links but simply by their first name. In order to take this into account we altered the way we extracted character links slightly. Instead of using regex to match for a certain syntax of links, we simply used regex to search through the text for all character names: both first name and first name + last name. . G_raw = nx.read_gpickle(&quot;../data/got_G.gpickle&quot;) # Create attribute dict for seasonal networks from the overall network attribute_dict = {} for node, attribute in G_raw.nodes(data = True): attribute_dict[node] = attribute . char_list_all = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] char_names_cap = [] # List of all characters for char in char_list_all: char_names_cap.append(char) for i in range(1,9): tf_idf_season = tf_idf_func(i) # Computation of most used words files = os.listdir(&quot;../data/got2/s&quot;+str(i) +&quot;/&quot;) # List of all character files for a given season char_list = [file.split(&#39;.txt&#39;)[0] for file in files] #List of all characters G = nx.DiGraph() # Create Directed Graph Object for file in files: #Iterate through character files for a given season with open(&quot;../data/got2/s&quot;+str(i)+&#39;/&#39; + file) as f: node_description = f.read() name_char = file.split(&quot;.txt&quot;)[0] # Special case if name_char == &quot;Royal_Steward&quot;: continue #pattern = &quot; [ [(.*?) ] ]&quot; # Pattern for extracting links # links = re.findall(pattern, node_description) #Extract character links by searching through txt file for character names. links = [] for name in char_names_cap: if len(re.findall(name, node_description)) &gt;0 : links.extend(re.findall(name, node_description)) elif len(name.split(&#39; &#39;))&gt;1: if len(re.findall(name.split(&quot; &quot;)[0], node_description)) &gt;1: tmp_link = re.findall(name.split(&quot; &quot;)[0], node_description) if len(tmp_link) &gt; len(re.findall(name, node_description)): links.extend([name] * (len(tmp_link) - len(re.findall(name, node_description)))) # Count of each unique link links, counts = np.unique([link.replace(&quot; &quot;, &quot;_&quot;) for link in links if link.replace(&quot; &quot;, &quot;_&quot;) in char_list], return_counts=True) # print(name_char) # Saving attributes for network app attributes = attribute_dict[name_char] name_for_text = name_char.replace(&#39;_&#39;, &#39; &#39;).capitalize() if len(name_for_text.split(&#39; &#39;))&gt; 1: name_for_text = name_for_text.split(&#39; &#39;)[0].capitalize() + &#39; &#39; + name_for_text.split(&#39; &#39;)[1].capitalize() attributes[&#39;text&#39;] = &quot;The selected character: &quot; + name_for_text + &quot; has the following attributes: &quot;+ &quot; n - Allegiance: &quot; + attributes[&#39;allegiance&#39;] + &quot; n - Religion: &quot; + attributes[&#39;religion&#39;] + &quot; n - Culture: &quot;+attributes[&#39;culture&#39;] + &quot; nMost used words by &quot; + name_for_text + &quot; are: &quot; + &quot;, &quot;.join([word[0] for word in sorted(tf_idf_season[name_char].items(), key=lambda value: value[1], reverse = True)[:5]]) + &quot; nLink to character page are: https://gameofthrones.fandom.com/wiki/&quot;+ name_char.replace(&#39; &#39;, &#39;_&#39;) attributes[&#39;thumbnail&#39;] = get_img(name_char) attributes[&#39;wordcloud&#39;] = &quot;&quot; # Create node for given character with extracted attributes G.add_node(name_char, **attributes) # Add edges to node final_links = [] for j, link in enumerate(links): final_links.append((name_char,link,counts[j])) G.add_weighted_edges_from(final_links) # Save .gpickle file containing the network. nx.write_gpickle(G, &quot;../data/got_G_s&quot;+str(i)+&quot;.gpickle&quot;) . 2.1.2 The Dialogue of Game of Thrones . The second data set used in this project is all the dialogue in the Game of Thrones series. This data set can be found here. This data set contains, for each episode in the series, the episode name and all the dialogue for the given episode. As this data is already in .json format it&#39;s simply loaded in, and we create a dictionary of characters and all their dialogue throughout the show. . resp = requests.get(&quot;https://raw.githubusercontent.com/jeffreylancaster/game-of-thrones/master/data/script-bag-of-words.json&quot;) diag = json.loads(resp.text) char_diag = {} for element in diag: episode = element[&#39;episodeNum&#39;] season = element[&#39;seasonNum&#39;] title = element[&#39;episodeTitle&#39;] text = element[&#39;text&#39;] for textObj in text: if textObj[&#39;name&#39;] in char_diag: char_diag[textObj[&#39;name&#39;]].append(textObj[&#39;text&#39;]) else: char_diag[textObj[&#39;name&#39;]] = [textObj[&#39;text&#39;]] . No pre-processing was needed for this dataset. . 2.1.2 IMBd ratings and reviews . The last data set we utilized is the episodes of the show IMDb rating and reviews. This was extracted using the python library IMDbPY. This data contains the average ratings of each episode and demographic information for each reviewer such as gender and age. In order to utilize the IMDbPY package, the IMDb ID for each episode was needed. This was extracted manually: . imdb_ids = {&#39;season 1&#39;: [&quot;1480055&quot;, &quot;1668746&quot;, &quot;1829962&quot;, &quot;1829963&quot;,&quot;1829964&quot;,&quot;1837862&quot;,&quot;1837863&quot;,&quot;1837864&quot;,&quot;1851398&quot;, &quot;1851397&quot;], &#39;season 2&#39; : [&quot;1971833&quot;,&quot;2069318&quot;,&quot;2070135&quot;,&quot;2069319&quot;,&quot;2074658&quot;,&quot;2085238&quot;,&quot;2085239&quot;,&quot;2085240&quot;,&quot;2084342&quot;, &quot;2112510&quot;], &#39;season 3&#39; : [&quot;2178782&quot;,&quot;2178772&quot;,&quot;2178802&quot;,&quot;2178798&quot;,&quot;2178788&quot;,&quot;2178812&quot;,&quot;2178814&quot;,&quot;2178806&quot;,&quot;2178784&quot;, &quot;2178796&quot;], &#39;season 4&#39; : [&quot;2816136&quot;,&quot;2832378&quot;,&quot;2972426&quot;,&quot;2972428&quot;,&quot;3060856&quot;,&quot;3060910&quot;,&quot;3060876&quot;,&quot;3060782&quot;,&quot;3060858&quot;, &quot;3060860&quot;], &#39;season 5&#39; : [&quot;3658012&quot;,&quot;3846626&quot;,&quot;3866836&quot;,&quot;3866838&quot;,&quot;3866840&quot;,&quot;3866842&quot;,&quot;3866846&quot;,&quot;3866850&quot;,&quot;3866826&quot;, &quot;3866862&quot;], &#39;season 6&#39; : [&quot;3658014&quot;,&quot;4077554&quot;,&quot;4131606&quot;,&quot;4283016&quot;,&quot;4283028&quot;,&quot;4283054&quot;,&quot;4283060&quot;,&quot;4283074&quot;,&quot;4283088&quot;,&quot;4283094&quot;], &#39;season 7&#39; : [&quot;5654088&quot;,&quot;5655178&quot;,&quot;5775840&quot;,&quot;5775846&quot;,&quot;5775854&quot;,&quot;5775864&quot;,&quot;5775874&quot;,], &#39;season 8&#39; : [&quot;5924366&quot;,&quot;6027908&quot;,&quot;6027912&quot;,&quot;6027914&quot;,&quot;6027916&quot;,&quot;6027920&quot;] } . Next, we initiate an instance of the IMDb class and iterate through all the episode IDs listed above. We then extract the reviews for each episode and the rating votes. Unfortunately, using this method we were limited to only recieve 25 reviews for each episode. The extracted information is then stored in a dictionary and saved as a .json file as can be seen below. . ia = IMDb() #Extract reviews: imdb_reviews = {} for season, episodes in imdb_ids.items(): imdb_reviews[season] = {} for episode in episodes: reviews = ia.get_movie_reviews(episode) votes = ia.get_movie_vote_details(episode) print(len(reviews[&#39;data&#39;][&#39;reviews&#39;])) imdb_reviews[season][episode] = {&quot;reviews&quot; : reviews[&#39;data&#39;][&#39;reviews&#39;], &quot;ratings&quot; : votes[&#39;data&#39;]} with open(&#39;../data/imdb_reviews.json&#39;,&#39;w+&#39;) as f: json.dump(imdb_reviews, f, indent = 4) . Again, no pre-processing of this data was needed. . 2.2 Basic statistics . In this section we are going to investigate the basic statistics of the Game Of Thrones network build on the full data from the wiki-pages which can be found here. Further, are we going to investigate the properties of the Game Of Thrones dialogoue which can be found here. Lastly, we are going to investigate the reviews and ratings data from IMDB which were gathered using a python package. The data can be found here. . Each of these exploratory analysis are going to have a subsection in order to make it easier to find it in this notebook. . 2.2.1 Wiki-page data . We are going to start out by loading the pickle file containing the network and all its attributes. The network contains the edges between all characters, and each node which are representing a character contains five attributes: status, appearances, culture, allegiance and religion which we are going to investigate further. . G = nx.read_gpickle(&quot;../data/got_G.gpickle&quot;) #Make space in a property dict for the values: property_dict = { &quot;status&quot;: [], &quot;appearances&quot;: [], &quot;culture&quot; : [], &quot;allegiance&quot;: [], &quot;religion&quot; : [] } #Allocate space for characters and define attributes: attributes = [&quot;status&quot;, &quot;appearances&quot;, &quot;culture&quot;, &quot;allegiance&quot;, &quot;religion&quot;] characters = [] . First are we going to investigate the number of characters and edges in the network. . Next, we are going to look into how the characters are distributed across the different allegiances, religions, cultures. How many characters die through the series, and how often does the characters occur. . print(f&#39;The network contains: {G.number_of_nodes()} nodes and {G.number_of_edges()} number of edges.&#39;) . The network contains: 162 nodes and 3085 number of edges. . for x,y in G.nodes(data = True): #Save the character name node_name = x #Append name to save all names. characters.append(node_name.replace(&#39;_&#39;, &#39; &#39;)) #For each of the five attributes save their value: for attribute in attributes: #A number of special handles if the value is empty then save No known XXX if attribute == &quot;appearances&quot;: if y[attribute] == &quot;&quot;: yat = 0 else: yat = int(y[attribute]) elif attribute == &quot;allegiance&quot;: if y[attribute] == &quot;&quot;: yat = &quot;No known allegiance&quot; else: yat = y[attribute] elif attribute == &quot;culture&quot;: if y[attribute] == &quot;&quot;: yat = &quot;No known culture&quot; else: yat = y[attribute] elif attribute == &quot;status&quot;: if y[attribute] == &quot;Place = [[Haystack Hall&quot; or y[attribute] == &#39;&#39;: yat = &#39;Unknown status&#39; else: yat = y[attribute] else: yat = y[attribute] #Save the attribute in the property_dict for plotting: property_dict[attribute].append(yat) . Now that we have saved all the attributes and character names we are ready for plotting it. This will be done using plotly to make the figures interactive. All figures are saved in a dict so after making the figures we can visualize them one by one. . df_property = pd.DataFrame.from_dict(property_dict, orient = &quot;columns&quot;) #Allocate space for all dataframes and figures: dfs = {} figs ={} #Iterate through all 5 attributes: for attribute in attributes: #Count number of occurences of the categories for the given attribute: dfs[attribute] = df_property[attribute].value_counts() #Reset index to have the attribute values as column instead of index: dfs[attribute] = dfs[attribute].reset_index() #Convert eg. status to Status for niceness in plot: dfs[attribute].columns = [attribute.capitalize(), &quot;Counts&quot;] #Special handling for appearances as we dont want to see zeros: if attribute == &#39;appearances&#39;: dfs[&#39;appearances&#39;] = dfs[&#39;appearances&#39;][dfs[&#39;appearances&#39;][&#39;Appearances&#39;] != 0] #Make and save figure: figs[attribute] = px.bar(dfs[attribute], x=attribute.capitalize(), y=&quot;Counts&quot;, color=attribute.capitalize(), title=&quot;Distribution of character &quot;+attribute) . Next, we are going to look at the plots one by one: . figs[&quot;religion&quot;].show() . From this it is apparent that a lot of the characters does not have a known religion, which is the majority of the characters, but if we toggle this of, we can see that the majority of the characters are part of Faith of The Seven and Old Gods of the Forest. On the other side it should be noted that the least frequent religions are White Walkers and Ghiscari religion. From the basic knowledge of the Game Of Thrones universe it also makes sense that the two most popular religions are Faith of The Seven and Old Gods of the Forest as the The Seven Kingdoms are practicing the Faith of the Seven whereas the people in the North are practicing the Old Gods of the Forest. . Further it should be noted that the the Game Of Thrones universe contains 8 different religions based on the Wiki pages. . figs[&quot;allegiance&quot;].show() . Again, some of the characters does not have an associated allegiance. The two most frequent allegiances are House Stark and Hose Lannister, followed by Night&#39;s Watch and House Targaryen. These allegiances are also the main allegiances in Game Of Thrones and further also the allegiances of the main characters in the series. . House Lannister has characters as Cersei, Jamie and Tyrion whereas House Stark has Robb, Bran and the bastard Jon Snow. Jon Snow is one of the series most well known character which is also part of the Night&#39;s Watch, and the the Night&#39;s Watch are playing a big role later in the series when the battle against the White Walkers are happening. Lastly, House Targaryen are a house which is beaten down but as the series are evolving Daenerys are becoming a larger player in the universe as she conquers the world part by part. . figs[&quot;culture&quot;].show() . From the above figure it can be seen that the most prominent culture are Andals followed by Northmen, again a large group has a unknown culture. From this it is apparent that most of the characters are found in the Andals and Northmen cultures, and makes the majority of the Game Of Thrones universe. Further, it should be noted that the universe contains a lot of small cultures such Children of the Forest. . The Andals are the people who invaded Westeros in the beginning of the universe, and are the dominant group. The Northmen are also a big cultural group defined by all the characters living in the North of the Game Of Thrones world. The Children of the Forest are a small group of characters which are presented fairly late in the series. They are small non-human characters, and should be the original people of Westeros. Further it should be noticed that the network contains a lot of different cultures. . figs[&quot;status&quot;].show() . From the plot we can see that 121 characters dies throughout the series, and anyone who has seen the series would be able to confirm that a lot of characters die as the series progresses. In the figure below the distribution of the number of episode appearences can be seen. . figs[&quot;appearances&quot;].show() . Again, a lot of characters do not have this attribute on their character page, and these observations have been omitted in the figure above. We can see that the majority of the characters only appear a couple of times ie. below 10-15 apperances. This would make sense as a lot of the characters are not main characters and therefore only appear in a season or likewise. We can further see a little group around 40 appearances and 60 appearances which could indicate we have a little group of characters appearing in most episodes, which would be expected as the series have a couple of main characters. . 2.2.2 Character dialogoue . Next, we going to do some exploratory analysis of the character dialogoue. The dialogoue comes in a list of json objects, which each contains the information which episode, season it comes from. Each episode object does also contain a list of objects which contains a name of the character speaks and then the given dialogoue of the character. . We start out by loading the dialogoue from a github page. . resp = requests.get(&quot;https://raw.githubusercontent.com/jeffreylancaster/game-of-thrones/master/data/script-bag-of-words.json&quot;) diag = json.loads(resp.text) #Allocate space for character dialog: char_diag = {} char_count = {} . Next, we are going to pre-process the dialogoue into two dictionaries, namely a dictionary char_diag that contains the character name as a key, the value of the dictionary is the dialogoue of the character. . The other dictionary char_count contains the number of occurences for the given character at a season level and episode level. This are used to investigate the number of appearances of each character. Further, we are saving the length of the given character dialogoue. . for element in diag: #Extract season, episode number and the title of the episode: episode = element[&#39;episodeNum&#39;] season = element[&#39;seasonNum&#39;] title = element[&#39;episodeTitle&#39;] #Create new object with the text: text = element[&#39;text&#39;] #Iterate through each object in the text object which contains all dialogoue of the episode: for textObj in text: #If the character name are not in our character network then continue: if textObj[&#39;name&#39;] not in characters: continue #If the character already are in the dictionary: if textObj[&#39;name&#39;] in char_diag: #Then append text: char_diag[textObj[&#39;name&#39;]].append(textObj[&#39;text&#39;]) #Put the episode into the character count dictionary - to keep track of which seasons and episodes: if (&quot;S&quot;+str(season)+&quot;E&quot;+str(episode)) not in char_count[textObj[&quot;name&quot;]][&#39;episodes&#39;]: char_count[textObj[&quot;name&quot;]][&#39;episodes&#39;].append(&quot;S&quot;+str(season)+&quot;E&quot;+str(episode)) if season not in char_count[textObj[&quot;name&quot;]][&#39;seasons&#39;]: char_count[textObj[&#39;name&#39;]][&#39;seasons&#39;].append(season) #Save the length of the dialogoue: char_count[textObj[&#39;name&#39;]][&#39;diag&#39;] += len(word_tokenize(textObj[&#39;text&#39;])) else:#If not already present in the object add it: char_diag[textObj[&#39;name&#39;]] = [textObj[&#39;text&#39;]] char_count[textObj[&#39;name&#39;]] = {&#39;episodes&#39;: [&quot;S&quot;+str(season)+&quot;E&quot;+str(episode)], &#39;seasons&#39;: [season], &quot;diag&quot;: 0} . Having saved the data in two dictionaries we are going to convert the char_countinto a dataframe which makes it more easy to plot afterwards. . Simultaneously, we count the number of unique occurences across episodes, seasons for each character. . df_diag = pd.DataFrame( #Iterate through all characters: {&quot;character&quot; : [char for char in char_count.keys()], #Count number of episodes the character are present in: &#39;Character episode count&#39;: [len(v[&#39;episodes&#39;]) for char, v in char_count.items()], #Count number of seasons the character are present in: &#39;Character season count&#39;:[len(v[&#39;seasons&#39;]) for char, v in char_count.items()], #Add total dialogoue length for each character: &#39;Character diag length&#39;: [v[&#39;diag&#39;] for char, v in char_count.items()] }) . We are now ready to plot the 3 dialogoue attributes, this are again done using plotly and are going to be done using bar-plots. . fig_char_diag_len = px.bar(df_diag,x = &quot;character&quot;, y=&quot;Character diag length&quot;, title = &quot;Character dialogoue length distribution&quot;) fig_char_diag_len.update_layout(xaxis={&#39;categoryorder&#39;:&#39;total descending&#39;}) fig_char_diag_len.show() . From the figure above we can see that Tyrion Lannister clearly are the character with the longest dialogoue, which for anyone who has seen the series knows that Tyrion talks a lot and likes to talk. Next we can see that Jon Snow, Cersei Lannister and Daenerys Targaryen also has a lot of dialogoue. This makes sense as these three are part of the main characters, and appear in a lot of episodes. . fig_char_season = px.bar(df_diag,x = &quot;character&quot;, y=&quot;Character season count&quot;, title = &quot;Character appearances distribution (season level)&quot;) fig_char_season.update_layout(xaxis={&#39;categoryorder&#39;:&#39;total descending&#39;}) fig_char_season.show() . From the figure above it can be seen that a lot of characters are present in all 8 season such as: Jon Snow, Sansa Stark, Tyrion Lannister, Bronn and Samwell Tarly and again this is expected as these characters are part of the key characters. On the other side a lot of characters are only present in 1 season such as Syrio Forel which is Arya Starks &quot;dancing teacher&quot; when she moves to King&#39;s Landing. . fig_char_episode = px.bar(df_diag,x = &quot;character&quot;, y=&quot;Character episode count&quot;, title = &quot;Character appearances distribution (episode level)&quot;) fig_char_episode.update_layout(xaxis={&#39;categoryorder&#39;:&#39;total descending&#39;}) fig_char_episode.show() . From this we can see that the character which appear in most episodes are Tyrion Lannister followed by Jon Snow, Sansa Stark, Daenerys Targaryen which makes perfect sense as these characters are main characters. Only a couple of characters are present only ones which clearly would indicate they had a small role in the Game Of Thrones plot. . 2.2.3 Reviews and ratings . As a last data-source we are looking into the reviews and ratings taken from IMDB, though using a python package (see earlier section) as it is illegal to scrape the IMDB webpage which we otherwise was looking into. . We are going to look into the average rating at episode and season level. Also we are looking into the demographics of the raters/reviewers, and lastly, what is the average review length for each episode/season. . We start out by loading the data which is saved in a .json file. . f = open(&quot;../data/imdb_reviews.json&quot;) ratings = json.load(f) episode_rating = {} season_rating = {} . Now, we want to organize the data to ease the plotting afterwards. The ratings dictioanary is organized in a manner where the upper level key is the season, next the episode, and in each episode it contains the reviews and the ratings. This we want to extract and organize so it is more easy to plot. . We create two new dictionaries: one for storing the rating on season level and another for storing the rating on episode level. . s = 0 #Iterate through each season: for season, episodes in ratings.items(): season_rating[&quot;S&quot; + str(s+1)] = 0 c = 0 #Iterate throug all episodes in this season: for episode in episodes: #Add rating: season_rating[&quot;S&quot;+ str(s+1)] += episodes[episode][&#39;ratings&#39;][&#39;demographics&#39;][&#39;imdb users&#39;][&#39;rating&#39;] episode_rating[&quot;S&quot; + str(s+1) + &quot; E&quot; + str(c+1)]= episodes[episode][&#39;ratings&#39;][&#39;demographics&#39;][&#39;imdb users&#39;][&#39;rating&#39;] c+= 1 #Divide by number of episodes in this season: season_rating[&quot;S&quot;+ str(s+1)] = season_rating[&quot;S&quot;+str(s+1)]/c s+=1 . Now that we have organized the data we are ready for investigating the average rating pr. season and episode. . df_season_rating = pd.DataFrame.from_dict(season_rating, orient= &#39;index&#39;) #Reset index so we have a column named season instead: df_season_rating = df_season_rating.reset_index() df_season_rating.columns = [&#39;Season&#39;, &quot;IMDB rating&quot;] #Plot it: fig_season_rating = px.bar(df_season_rating, x=&quot;Season&quot;, y=&quot;IMDB rating&quot;, color=&quot;Season&quot;, title=&quot;Rating pr. season&quot;) fig_season_rating.show() . Next we will look at the average rating pr. episode, to see if we could find any patterns. From the figure below we see approximately the same pattern as above, but we can now see that often the last 2 episodes in a season do achieve a higher average score compare to the middle episodes. Further it should be noticed that from the beginning of season 8 the episodes do keep getting lower average score, and the last episode in season 8 do achieve a quite low score of only 4. . df_episode_rating = pd.DataFrame.from_dict(episode_rating, orient= &#39;index&#39;) #Reset index again: df_episode_rating = df_episode_rating.reset_index() df_episode_rating.columns = [&#39;Episode&#39;, &quot;IMDB rating&quot;] #Plot: fig_episode_rating = px.bar(df_episode_rating, x=&quot;Episode&quot;, y=&quot;IMDB rating&quot;, color=&quot;Episode&quot;, title=&quot;Rating pr. episode&quot;) fig_episode_rating.show() . Next part, investigate the demographics of the people who review and rate the Game Of Thrones series. In order to do this more easy we are going two create two functions for this. . One which extract the number of votes and rating sum, and another that reweights the rating. . def rating_weighted(cat_dist): &quot;&quot;&quot; Function that given a distribution of votes and rating_sum convert the rating into a weighted average &quot;&quot;&quot; for key, value in cat_dist.items(): #Iterate through each category and compute the weighted rating score: cat_dist[key][&#39;rating_sum&#39;] = value[&#39;rating_sum&#39;]/value[&#39;votes&#39;] return cat_dist def dist(item, dict_item): &quot;&quot;&quot; Function that iterates through a dict and assign the number of votes and rating sum &quot;&quot;&quot; for i in item.keys(): #If not in dict create the object: if i not in dict_item: dict_item[i] = {&#39;votes&#39;:0, &quot;rating_sum&quot;:0} #Put values into dict: dict_item[i][&quot;votes&quot;] += item[i][&#39;votes&#39;] dict_item[i][&quot;rating_sum&quot;] += item[i][&#39;rating&#39;]*item[i][&#39;votes&#39;] return dict_item cat_dist = {} #Iterate through each season: for season, episodes in ratings.items(): s = &quot;S&quot; + season.split(&quot; &quot;)[1] #Iterate through each episode in season: for c,(episode) in enumerate(episodes): e = &quot;E&quot; + str(c) #Get demographics iteM item = ratings[season][episode][&#39;ratings&#39;][&#39;demographics&#39;] #Extract values into cat_dist dictionary: cat_dist = dist(item, cat_dist) #Compute weighted average rating: cat_dist = rating_weighted(cat_dist) . Now that we have organized our data in a more organized manner that makes it more easy to plot we are going to plot the demographics of the voters. . x = [&quot;Aged under 18&quot;, &quot;Aged 18 29&quot;,&#39;Aged 30 44&#39;,&#39;Aged 45 plus&#39;]*2 #Categories for females: x_f = [&#39;females aged under 18&#39;, &#39;females aged 18 29&#39;, &#39;females aged 30 44&#39;,&#39;females aged 45 plus&#39;] #Categories for men: x_m = [&#39;males aged under 18&#39;, &#39;males aged 18 29&#39;, &#39;males aged 30 44&#39;,&#39;males aged 45 plus&#39;] #Get the gender: gender = np.concatenate(([&#39;female&#39;]*int(len(x)/2),[&#39;male&#39;]*int(len(x)/2))) x_gender = np.concatenate((x_f, x_m)) # Extract number of votes: v = [cat_dist[vote][&#39;votes&#39;] for vote in x_gender] #Extract average rating: ratings = [cat_dist[vote][&#39;rating_sum&#39;] for vote in x_gender] #Convert into dictionary for easy plotting: df = pd.DataFrame({ &quot;Category&quot;: x, &quot;Gender Category&quot; : x_gender, &quot;Gender&quot; : gender, &quot;Votes&quot; : v, &quot;Ratings&quot; : ratings }) #Plot number of votes based on age and gender: px.bar(df, x=&quot;Category&quot;, y=&quot;Votes&quot;, color=&quot;Gender&quot;, title=&quot;Distribution of number votes across age and gender&quot;) . Apperently the largest gender group is males which give votes, and the largest age group is age 30-44 whereas the smallest is under 18 years old. This would make sense as the series is restricted to 18 years or more. Further, it can be seen that people above age 45 does not tend to watch Game Of Thrones as much. . fig = go.Figure(data=[ go.Bar(name=&#39;Females&#39;, x=df[&#39;Category&#39;].unique(), y=df[&#39;Ratings&#39;][df[&#39;Gender&#39;] ==&#39;female&#39;]), go.Bar(name=&#39;Males&#39;, x=df[&#39;Category&#39;].unique(), y=df[&#39;Ratings&#39;][df[&#39;Gender&#39;] ==&#39;male&#39;]) ]) # Change the bar mode fig.update_layout(barmode=&#39;group&#39;,title_text=&quot;Distribution of average rating across age and gender&quot;, yaxis_title= &quot;Average IMDB rating&quot;, xaxis_title = &quot;Age category&quot;) fig.show() . Generally the average IMDB rating are quite constant across the age and gender groups. The average rating are further quite high as the IMDB rating score goes from 0-10 and the average is above 8. The group that gives the series the lowest score are females under 18 years, but this group is also rather small so not many people need to give it a bad rating before this score gets affected by the ratings. . 3. Tools, theory and analysis. Describe the process of theory to insight . . This section is going to describe how we have approached the task of answering the following questions about the Game Of Thrones: . Who is the main characters of each season of the series? | Is it possible to find a pattern in the data that helps understand the complicated world of Westeros? | Is the theme of the Game of Thrones series consistent throughout the show or does it change during it&#39;s course? | . This will be done first analysing the character network, next performing text analysis and lastly perform a community analysis. Therefore this section will be divided into three parts: . Network analysis | Text analysis | Community analysis | . Each part is going to be introduced below, as well as the subconclusion of each part. Further, a link to a notebook doing the full analysis of each of the parts will be provided. This is done to make this notebook more convenient, and as the size otherwise would be to big to render on Github, and on most computers be extremely slow to read through. . 3.1 Network analysis . The Notebook going through the full analysis can be found here. . This part analyse the character interactions, do people tend to group in eg. allegiances? Or do they interact with people from other allegiances? Which character are main character in each season and across the whole series? . This is questions that this part of the analysis are going to dive into. The methods used for this part of the analysis are mainly network science tools such as degree distributions, centrality measures, assortivity and lastly, good old exploratory analysis and visualizations in general. . The main outputs from this analysis are two inteactive visualizations/apps that the end user can play around with. The first app are based on directed graphs, where the links between the characters (nodes) are extracted from the links between the character pages on the Game Of Thrones wiki-page. The network app can be visualized both pr. season and across each season, further it can be overlayed with different colormaps based on selected attribute. . The other main output are a interactive visualization/table where the out- and in degrees and centrality measure for each character are presented in a table and a graph, where one can tab through the seasons. This can help determine the importance of each of the characters in the selected season. . 3.1.1 Subconclusion . The resulting graph across all seasons were a very dense graph, and through different network analysis tools and by splitting into seasons it was possible to determine the most important characters in each season. Further, we found that the character attributes selected (religion, culture, allegiance, appearance and status) did not pose to be good measures to distinguish the characters. The characters were possibly connected base on another attribute or underlying pattern. . In the next sections we are going to investigate community analysis and hopefully this can help us reveal some of these patterns. . 3.2 Text analysis . The Notebook doing the full text analysis can be found here. . . Do the theme of the Game Of Thrones change as the seasons goes by? Do characters and allegiances have specific words they use? Is it maybe possible to determine the mood of the characters throughout the series? This is some of the questions we try to answer and understand in this part of the analysis. . In order to dive into these questions the text analysis contains three parts namely: . TF-IDF analysis | Sentiment analysis | Dispersion plot | . The text analysis are based on two data sources namely data extracted from the wiki-pages (ie. the character pages), and from the character dialogoue which is based on transcripts (see data section for more in depth description). In the TF-IDF we analyse the words used by each character, and compare the results between the character dialogoue and wiki-page. Further, we dive into whether the different allegiances has specific words they used compare to other. For both character and allegiance text analysis a few characters and allegiances are selected and compared. Further, we look into whether the theme changes throughout the seasons, and are done by looking into the words used in each season. . The results are presented in wordclouds in order to make it easy to grasp the information, and get an overview. . Further, in order to determine the mood of the characters we look into sentiment analysis, here two methods are utilized for the analysis: LabMT and VADER, which will be further explained in the linked notebook. Lastly, in order to look into whether the theme changes we are going to visualize a couple of selected words in a dispersion plot, to look into whether these change. A few words that are carefully selected are investigated. . 3.2.1 Subconclusion . Through the use of wordclouds it was possible to dive into the words used by a few selected characters and allegiances, and the extracted words clearly did explain these characters and allegiances. The words did also correspond well with the biased image of the characters and allegiances. Further, through the TF-IDF analysis of the seasons, we could see that the theme clearly changes as the series progress, and this was further backed up by the dispersion plot. . Lastly, by analysing the sentiment of the characters based on the dialogoue and wiki-pages that the dialogoue did contain larger variation in sentiment which would be expected as dialogoue would be expected to contain more polarizing words. . 3.3 Community analysis . The Notebook doing the full community analysis can be found here). . What is the underlying pattern that connects the characters? This is the main question of this section. From the network analysis (in the previous part) it appeared that the underlying pattern connection the different characters was not primarily based on the selected attributes for each character. This section contains four parts which will dive into how the characters are connected. The sections are as follows: . Community detection and exploratory analysis | Network analysis | TF-IDF | Sentiment analysis | . We utilzing the Louvain algorithm to detect communities in our character network, which we are going to perform some exploratory analysis on, and afterwards create an interactive network visualization/app same approach as in the Network analysis section. Afterwards, we are going to dive into the words characterizing these communities and lastly, looking into the sentiment. . 3.3.1 Subconclusion . The Louvain algorithm did extract six communities, and through the exploratory analysis it became clear that the characters were not grouped by their attributes (allegiance, religion etc.) but instead based on which people they were sourounded by. Daenarys Targaryen was a good example of this. Further when we looked into the community networks this became even more clear that this was the underlying pattern connecting these characters. Which seemed obvious when you think about it. We further analyzed the words of each community and this further backed up the hypothesis, and lastly the sentiment analysis that these communities clearly had different levels of sentiment. . 4. Discussion . Our primary data source were the wiki-pages, and this proved to be a time consuming task to extract data from, due to a lot of human incosistencies and errors. This included multiple re-iterations of the regular expressions used to extract both character links, but also the attributes for each character. We did in the end after numerous attempts end up with a approach that appears to be mostly error free (we might find some when digging more into this), so we ended up with clean data. We did also discover that the information present for each character was very diverse and characters did contain different amount and quality of information, which needed to be handled. . In the beginning of the project we wished to add all the reviews from IMDB, which we in the beginning approached with web-scraping and used quite some time to set scripts and functions up to do this, but learned after some research that this was not allowed. We did therefore approach this by using a bult-in package in Python to extract these, but this showed to only extract 25 reviews pr. episode. After further analysis this showed not be enough as eg. the reviews and ratings not always appeared to be matching very well. We did therefore drop to use the reviews in this project, but we still think it could be interesting to investigate how sentiment of the reviews developed as the series progresses and compare this with the state of the (main) characters. Do we change our state of mood when the characters? Humans are said to mirror other humans, so this would be intersting to dive into. . . We especially think that our interactive visualization apps ie. the network app that enables the user to investigate the interactions season by season but also across the whole story went very well. This app contains enormous amount of information as one can get information about each character including the attributes, frequent used words, image and a link to the character page on the wiki-page. Further, the user can overlay the network with different attributes, and investigate patterns. At the same time we think the visualization are easy to understand without diving deep into the information. . The app describing the degrees and closeness centrality is also an element that we are proud of, as this can in an easy to grasp approach show what are the main character in each season. Also we are very proud of the large functionalities of this app, as the user can sort and delete features. . 5. Future Work . The data only contained 165 characters as the primary data source used for the character was from the Game Of Thrones wikipage, which did not contain all the characters from the books. It would be interesting to further investigate the characters, their interactions, and some of the underlying patterns as there yet are a lot to dive into. The data could be supplied with further data from the books which can be found here. . This extensive dataset could add further complexity to the network analysis, and further the dialogoues used in this project could suplement information of the dialogoue of the majority of these characters. The dialogoue dataset contains more than 800 characters, and supplying the to date analysis with these elements could hopefully help reveal further interesting patterns of the Game Of Thrones story. . . 6. Conclusion . To reiterate our goals of this project: . Create a visually nice and interactive website | To perform network analysis and investigate of any patterns arises in the network attributes | Determine who was actually the main character of the story and it&#39;s respective seasons | Determine the overall sentiment of each character in the Game of Thrones series | Search for communities in the Game of Thrones network and establish if these communities makes sense from a story point of view | . . In conclusion, we, subjectively, think we succeeced in creating a visually pleasing website, with opportunities for the viewer to explore the extraced data and analysis themselves. This was done by utilizing interactive Dash applications hosted on Heroku. A lot of effort went into the making of the website content and it&#39;s formatting. In our network analysis it was possible to determine the most important characters of the show and it&#39;s respective seasons. This was achieved by investigating centrality measures and degree distributions. It was also concluded that the extracted attributes of religion, culture and allegiance did not pose to be good measures to distinguish the characters. . We did further analyse characters and their allegiances through use of text analysis, especially we focused on TF-IDF scores which we further visualized using wordclouds. This clearly established properties of each characters and their allegiances, and was in line with what was expected from our biased opinion. Further, we looked into whether the overall theme of the series changed through the seasons, which we investigated using TF-IDF analysis and dispersion plot, from here it was possible to see new patterns in theme which clearly changed through the story. As a last element on the text analysis we looked into sentiment analysis and could clearly find differences in the mood of the characters and as well we found that sentiment analysis appeared to have the best effect when used on dialogoue, where we could see larger variation in the sentiment across characters. . Lastly, as the character attributes did not pose as a good measure to distinguish the characters we performed community detection and found that the best way to split the characters was based on which other characters that sorrounded them. This conclusion can appear obvious, but nevertheless this was our finding from the analysis. We did further investigate the properties of these communities in these were clearly inline with the main result that the characters are best split into communities based on which characters they are sorrounded by. . 7. Contributions . Both group members have contribute to all parts of the project. . Introduction: Mikkel . Data: Nicolai . Basic statistics: Mikkel . Network analysis: Nicolai . Text analysis: Mikkel . Community analysis: Nicolai . Network app: Mikkel . Table app: Nicolai . Community app: Mikkel . Main explainer notebook: Nicolai . Network analysis explainer notebook: Mikkel . Text analysis explainer notebook: Nicolai . Community analysis explainer notebook: Mikkel . Website: Nicolai . 8. References . Data sources: . Game of thrones Wikipedia: https://gameofthrones.fandom.com/wiki/Game_of_Thrones_Wiki . Dialogoue data: https://github.com/jeffreylancaster/game-of-thrones#data . Plotting and visualization templates: . Plotly: https://dash.plotly.com/ . Heroku: https://towardsdatascience.com/deploying-your-dash-app-to-heroku-the-magical-guide-39bd6a0c586c . Previous work in the 02805 Social Graphs and Interactions assignments . Website: . Fastai fastpages: https://github.com/fastai/fastpages . Network science: . Design &amp; Data Visualization: Max Tillich, Kim Albrecht, Mauro Martino, Marton Posfai and Gabriele Musella. . Sentiment analysis methods: . VADER: https://github.com/cjhutto/vaderSentiment . LabMT: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4466725/ . &lt;/div&gt; .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/Explainer_Notebook/",
          "relUrl": "/Explainer_Notebook/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Explainer Notebook - Network Analysis",
          "content": "Network Analysis . This notebook is going to dive into the full network analysis done in this project. This includes some basic stats of the network, how the interactive network visualization are created and further some properties of the network. . How the network is generated and how the information is extracted from the Game Of Thrones wiki-page is described in section 2.1.1 Data extraction, cleaning and preprocessing in the Explainer Notebook. . We will start out by importing the needed packages for performing the network analysis. . import networkx as nx import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import json import requests import os import contextlib import powerlaw import plotly.graph_objects as go . As the network have been saved in a gpickle file we can just load it in: . G = nx.read_gpickle(&quot;../data/got_G.gpickle&quot;) . The network we are mainly investigating is the full network based on all the information from each character page and not the separate networks for each season as these are often quite small. . 1. Basic stats of graph . Having loaded the network we will start out by investigating some basic stats of the network. If you want to look into the basic statistics behind the data you can look at section 2.2.1 Basic Statistics in the Explainer Notebook. . Lets start out by determining the size of the network with respect to number of nodes and edges ie. characters and links. Each node represents a character and has some assigned attributes namely: allegiance, religion, culture, status (dead, alive, uncertain) and number of appearances. Each link between two nodes are determined by the linking between the wiki-pages and if these links are directed. Further, each link has a frequency describing the number of occurences this specific link has. . print(f&#39;The network contains: {G.number_of_nodes()} nodes and {G.number_of_edges()} number of edges.&#39;) . The network contains: 162 nodes and 3085 number of edges. . The network contains 162 nodes ie. 162 characters and 3085 edges. This means that the network is very dense. One would start to look into whether all nodes are connected in one Giant Component or it contains some smaller networks. . We can use the weakly_connected_components to find the clusters in the network and then use subgraph to extract the largest component. . GCC = G.subgraph(max(nx.weakly_connected_components(G),key=len)) print(f&#39;The GCC contains: {GCC.number_of_nodes()} nodes and {GCC.number_of_edges()} number of edges.&#39;) . The GCC contains: 162 nodes and 3085 number of edges. . We notice that the network are the same, and this means we have a very dense network with all characters being connected. . 1.1 Average shortest path . Having determined that the network is very dense and the starting network also are the GCC, we are going to determine the average shortest path using the average_shortest_path_length function. . print(f&#39;Average shortest path is: {round(nx.average_shortest_path_length(G),3)}&#39;) . Average shortest path is: 2.162 . The average shortest path is just above two, which means that the average distance is only 2 steps, which again indicates how dense the network is. . 1.2 In- and out degree . We are starting out by examining the degree distribution of the network, as the network is a directed network this would include both the in degree and out degree distribution. The in-degree is the number of inward edges from a node to the given node and vice versa for the out-degree. The degree describes the number of edges, and can contain information describing the characters connectiveness in the network. . in_degrees = [G.in_degree(n) for n in G.nodes()] out_degrees = [G.out_degree(n) for n in G.nodes()] df = pd.DataFrame({ &quot;Degree&quot; : np.concatenate((in_degrees,out_degrees)), &quot;Degree type&quot; : np.concatenate(([&quot;In-degree&quot;]*len(in_degrees), [&quot;Out-degree&quot;]*len(out_degrees))), }) #Compute in- and out degree of nodes having role == Ally in_degrees = [[n,G.in_degree(n)] for n, d in G.nodes(data = True)] out_degrees= [[n,G.out_degree(n)] for n, d in G.nodes(data = True)] #Sort and select only top 5 most connected: top5_in_degrees = sorted(in_degrees, key = lambda i: i[1], reverse = True) top5_out_degrees= sorted(out_degrees, key = lambda i: i[1], reverse = True) fig = px.histogram(df, x=&quot;Degree&quot;,color = &#39;Degree type&#39;, marginal=&quot;box&quot;, title = &quot;Degree distribution&quot;) fig.show() . . . From the figure above it can be seen that the in-degree and out-degree distribution are very similar, and appears to come from the same distribution. It should be noted from both the histogram and the boxplot that the in-degree distribution has more extreme point but at the same time has a lower median. Further it can be seen that the out-degree (ie. outgoing links) appear to have a higher peak around 5-9 degree compared to in-degree. It should also be noticed that the network is very dense, and all characters have at least a couple of other characters they interact with. From the degree distribution we can find the top-5 most connected characters based on in- and out-degree which is presented in the tables below. . . . From the table above it appears that Jon Snow is the most connected character based on in-degree. Also it should be noted that all five characters are main characters in Game Of Thrones, and therefore it would make sense that they are well connected in the network. Further, all but Eddard Stark are characters that are appearing in most episodes (see Basic Statistics). . Eddark Stark dies quite early in the series, and it might be a surprise that he is one of the most connected characters, but as this is based on in-degree this could be due to many of the others characters pages references Eddard. This would make sense as his children probably talks about him/mentions him and therefore make him very connected compared to many other characters. . Based on out-degree again Jon Snow is the most connected character, but we see that Eddard Stark, Daeneras Targaryen and Cersei Lannister are replaced by Sansa Stark, Arya Stark and Jamie Lannister which also are very well connected characters and also appears as main characters in the series. . We will now try to investigate the in- and out-degree in a log-scale environment, as this sometimes can reveal the patterns. . in_degrees = [d for name, d in top5_in_degrees] hist_in, bins = np.histogram(in_degrees, bins = np.arange(np.min(in_degrees), np.max(in_degrees)+1)) #Compute center of each bin center_in = (bins[:-1] + bins[1:]) / 2 #Create bins and counts out_degrees = [d for name, d in top5_out_degrees] hist_out, bins = np.histogram(out_degrees, bins = np.arange(np.min(out_degrees), np.max(out_degrees)+1)) #Compute center of each bin center_out = (bins[:-1] + bins[1:]) / 2 plt.figure(figsize=(12,8)) plt.loglog(center_in, hist_in, &#39;.b&#39;, label = &quot;In degree&quot;,markersize = 12) plt.loglog(center_out, hist_out, &#39;.r&#39;, label = &quot;Out degree&quot;,markersize = 12) plt.xlabel(&#39;Degree&#39;, fontsize = 12) plt.ylabel(&#39;Count&#39;, fontsize = 12) plt.title(&#39;Degree distribution with log axis&#39;, fontsize = 20) plt.legend() plt.show() . This could look like a random network, but also somewhat like a scale-free network, and it is hard to determine exactly. Therefore we will look into powerlaw. This is done using the powerlaw package which determines the coefficient. . with open(os.devnull, &quot;w&quot;) as f, contextlib.redirect_stdout(f): #Compute exponent of in and out-degree distributions in_alpha = powerlaw.Fit([G.in_degree(n) for n in G.nodes()],verbose = False).alpha out_alpha= powerlaw.Fit([G.out_degree(n) for n in G.nodes()],verbose = False).alpha print(f&quot;Power law exponent for in degrees: {round(in_alpha,2)}&quot;) print(f&quot;Power law exponent for out degrees: {round(out_alpha,2)}&quot;) . Power law exponent for in degrees: 2.56 Power law exponent for out degrees: 3.03 . The out-degree coefficient is just above 3 indicating that the network is a random network, but it is quite close to be a scale-free network. The in-degree are below 3 and above 2 which indicates that it is scale-free. . Next, we looking into how the in- and out-degree are correlated, ie. if you are connected to many people in a out-going fashion are these people also connecting to you. . correlation = np.corrcoef([d for name, d in top5_in_degrees],[d for name, d in top5_out_degrees]) plt.figure(figsize = (12,8)) plt.scatter([d for name, d in top5_in_degrees],[d for name, d in top5_out_degrees], color = &#39;red&#39;) plt.title(f&#39;In-degree vs. Out-degree: rho = {round(correlation[1,0],3)}&#39;, fontsize = 20) plt.xlabel(&#39;In degree&#39;, fontsize = 12) plt.ylabel(&#39;Out degree&#39;, fontsize = 12) plt.show() . From the plot, we can see that we have a very strong correlation between these connectiveness. . 1.3 Attribute interactions . In this part we are going to investigate how the attributes relate and interact with each other, also it could be that some attributes eg. religion is more well connected than others. We will start out by examining the allegiance attribute. . def most_connected_on_attribute(att, n, restrict = False): attribute = nx.get_node_attributes(G,att) # Get connection between houses and also find most connected houses: connection_dict = {} house_connection = {} for ef, et in list(G.edges()): a_from = attribute[ef] a_to = attribute[et] if a_from == &quot;&quot;: continue if a_to == &quot;&quot; : continue if &quot;No known&quot; in a_from: continue if &quot;No known&quot; in a_to: continue if a_from in connection_dict.keys(): house_connection[a_from] += 1 if a_to in connection_dict[a_from]: connection_dict[a_from][a_to] += 1 else: connection_dict[a_from][a_to] = 1 else: house_connection[a_from] = 1 connection_dict[a_from] = {a_to : 1} most_connected_houses = sorted(house_connection, key = lambda i: house_connection[i],reverse = True)[:n] tmp = [[house, house_connection[house]] for house in most_connected_houses] df_count = pd.DataFrame(tmp, columns = [att.capitalize(), &#39;Count&#39;]) fig_count = px.bar(df_count, x = att.capitalize(), y = &#39;Count&#39;, title = &quot;Most connected &quot;+ att) df_connection = pd.DataFrame.from_dict(connection_dict) df_connection = df_connection.fillna(0) if restrict: df_connection = df_connection.loc[most_connected_houses,most_connected_houses] fig_heatmap = px.imshow(df_connection, title = &quot;Heatmap of connection between &quot; + str(att)+&quot;s&quot;) return fig_count, df_count, fig_heatmap, df_connection fig_count, df_count, fig_heatmap, df_connection = most_connected_on_attribute(&quot;allegiance&quot;,8 ,True) fig_heatmap.show() . . . The figure above shows the top 10 most connected allegiances in Game Of Thrones, and it can clearly be seen that House Stark is the most connected allegiance, but a lot of the connectivity derives from the interaction with their own allegiance. Further, it can be seen that House Stark is well connected with House Lannister, Night&#39;s Watch and House Bolton. The connection with House Lannister and Night&#39;s Watch can easily be explained by eg. Ned Starks&#39; work as the Kings Hand but also Sansa Stark being married with Joffrey. Further, Jon Snow from the House Stark allegiance are becoming part of Night&#39;s Watch can explain this interaction. . Generally, it can also be seen that the allegiances interacts with it-self most, compared to interaction with other allegiances. . Next, we will look into how the religions interact, and from the figure below, it can clearly be seen that the two main religions are the religions that mainly interact with each other, which does not come as a surprise. . fig_count, df_count, fig_heatmap, df_connection = most_connected_on_attribute(&quot;religion&quot;,5 ,True) fig_heatmap.show() . . . In the figure below, the connection between cultures are investigated. Here it can be seen that the Andals are the most connected culture, followed by Northmen. Furthermore, it can be seen that these two cultures interact alot. . It can be seen that the third most connected culture is Valyrians which does not mainly interact with themselves, but instead are most connected to Andals. . fig_count, df_count, fig_heatmap, df_connection = most_connected_on_attribute(&quot;culture&quot;,3 ,True) fig_heatmap.show() . . . 2. Visualization of the graph . This section is devoted to explain how we created our interactive visualization of the network. The app can be found by following this link. The code used to generate the network is found in two separate python files. The file for processing the Game Of Thrones gpickle file and creating links and edges can be found here, whereas the python file for the interactive visualization are found here. . The visualization contains a network which is created using plotly. Here the alphas and node sizes are scaled according their respective degree, whereas the edge width are scaled according to its frequency. This is done to indicate which nodes are well-connected and has a lot of connections, and further to make the visualizations more easy to understand, as the network is very dense, and can get confusing with all the links. . The positions of the nodes are computed using ForceAtlas2 algorithm, where the specific settings can be found in the compute_positions function. Further, these positions are saved to speed up the app, as it otherwise needed to recompute these every time, and this also make the figure more reproducible. . Besides having the network, below are also four &quot;boxes&quot; with information namely: Season selection, Graph attribute overlay, Click data and Character image. . Season selection makes it possible to choose whether the network should be based on all data across all seasons for the characters, or whether one wishes to look into a specific season. This makes it possible to investigate which characters are important in each season or across all seasons. . Graph attribute overlay are a functionality where one can change the node colour, which is based on the selected attribute. One can select whether the colouring of the nodes should be based on: allegiance, religion or culture. . Click data displays further information about the character when one click on a node. This information is allegiance, culture and religion of the selected attribute, and further a link to the character wiki-page. Further, the top 5 words based on TF-IDF are displayed, how these are computed can be seen in Text analysis - Notebook. . Lastly, Character image, as the name says displays the character image which is found using BeatifulSoup4 which scrapes the character wiki-page and finds the thumbnail image. . The app is hosted through Heroku, which for free-accounts only have limited ressources, so it might be slow if too many people access it at once. Lastly, the data used to create this visualization is described in the explainer notebook section 2.1.1 where the data are processed and cleaned and saved in gpickle files. One file for each season and a file for data across all seasons. The data files can be found here. . The app can be seen below: . 3. Network properties . In this section we will look further into properties of the network. This will include clustering, assoritivity and centrality of the network. Which will be described in the sections below. . Again, only the full network containing all data across seasons is going to be investigated in this section. . 3.1 Average clustering . We will start out by computing the average clustering coefficient for our network. This can help us understand how many of the possible links each character on average is connected to. . print(f&#39;The average clustering coefficient is: {nx.average_clustering(G)}&#39;) . The average clustering coefficient is: 0.48300137580612906 . From this we can see that on average each character is connected to little less than half of the possible links. This would be expected to be pushed up by some of the main characters and down for some of the less important characters. . plt.figure(figsize = (12,8)) plt.scatter([e for n, e in top5_in_degrees],nx.clustering(G).values(), color= &#39;red&#39;, label = &#39;In degree&#39;) plt.scatter([e for n, e in top5_out_degrees],nx.clustering(G).values(), color= &#39;blue&#39;, label = &#39;Out degree&#39;) plt.title(&#39;Clustering against degree&#39;, fontsize = 20) plt.xlabel(&#39;Degree&#39;, fontsize = 12) plt.ylabel(&#39;Clustering&#39;, fontsize = 12) plt.legend() plt.show() c_in = np.corrcoef([d for name, d in top5_in_degrees],[v for v in nx.clustering(G).values()]) c_out = np.corrcoef([d for name, d in top5_out_degrees],[v for v in nx.clustering(G).values()]) print(f&#39;Correlation with in degree: {round(c_in[1,0],3)}&#39;) print(f&#39;Correlation with out degree: {round(c_out[1,0],3)}&#39;) . Correlation with in degree: -0.377 Correlation with out degree: -0.412 . From the figure above, we can see a slight (very slight) negative correlation between clustering coefficient and the degree (in or out) and the correlation coefficient is approximately -0.5. Meaning character with high in or out-degree has a lower clustering coefficient. . 3.2 Assortivity . Next we are looking into assortivity of each of our character attributes to see if any of these could indicate whether the characters are linked based on their attribute. This could also help us understand how they are connected, and maybe how we could discover communities in our network. . A, assor = [], [] attributes = [&quot;religion&quot;, &quot;appearances&quot;, &quot;culture&quot;, &quot;allegiance&quot;] for attribute in attributes: A.append(attribute.capitalize()) assor.append(np.round(nx.attribute_assortativity_coefficient(G, attribute),3)) fig = go.Figure(data=[go.Table(header=dict(values=[&#39;&lt;b&gt;Attribute&lt;/b&gt;&#39;, &#39;&lt;b&gt;Assortivity&lt;/b&gt;&#39;]), cells=dict(values=[A, assor])) ]) fig.update_layout( height=300, showlegend=False, title_text = &quot;Assortivity score of each character attribute&quot; ) fig.show() . . . From the table above it can be seen that religion has the highest assortivity score, which would indicate that this attribute is the best to distinquish the characters from each other. It should be noted that none of the scores are very high indicating, that the characters are linked in a more complex pattern, or based on another attribute. . 3.3 Centrality . This section is going to investigate different measures of centrality of the full network across all seasons. . 3.3.1 Closeness centrality . We are further going to investigate the closeness centrality of each node in the network, which measures the reciprocal sum of shortests paths from the given node to all other nodes. If a node therefore has a high closeness centrality score this means that the node is close to the rest and vice versa. This could give us an indication of well connected characters and further important characters. . sort_cent = sorted(nx.closeness_centrality(G).items(), key=lambda kv: kv[1], reverse = True) fig = go.Figure(data=[go.Table(header=dict(values=[&#39;&lt;b&gt;Character&lt;/b&gt;&#39;, &#39;&lt;b&gt;Centrality score (closeness)&lt;/b&gt;&#39;]), cells=dict(values=[ [name[0].replace(&#39;_&#39;, &#39; &#39;) for name in sort_cent], [np.round(name[1],3) for name in sort_cent] ])) ]) fig.update_layout( height=310, showlegend=False, title_text = &quot;Closeness centralitiy score of each character&quot; ) fig.show() . . . In the table above the closeness centrality score is computed for all characters and sorted in descending order. As expected it can be seen that Jon Snow, Daenerys Targaryen and Tyrion Lannister are some of the characters close to the others. Further, Stannis Baratheon has a high centrality score, which would make sense as he is involved both as an heir to the throne, but also his involment with the red priest Mellisandre. . Further, Gregor Clegane, Eddard Stark, Bronn and the Night King has a high centrality score, which does not come as a surprise as these are key characters in the story, and interacts with many characters. In the bottom some of the smaller characters such as Ray and Aeron Greyjoy are present. . From this it could appear that character with a high degree also has a high closeness centralitity. This are we going to visualize in a scatter plot below and as well compute correlation coefficients for both in- and out-degree. . plt.figure(figsize = (12,8)) plt.scatter([e for n, e in top5_in_degrees],nx.closeness_centrality(G).values(), color= &#39;red&#39;, label = &#39;In degree&#39;) plt.scatter([e for n, e in top5_out_degrees],nx.closeness_centrality(G).values(), color= &#39;blue&#39;, label = &#39;Out degree&#39;) plt.title(&#39;Closeness centrality vs. degree&#39;, fontsize = 20) plt.xlabel(&#39;Degree&#39;, fontsize = 12) plt.ylabel(&#39;Closeness centrality&#39;, fontsize = 12) plt.legend() plt.show() c_in = np.corrcoef([d for name, d in top5_in_degrees],[v for v in nx.closeness_centrality(G).values()]) c_out = np.corrcoef([d for name, d in top5_out_degrees],[v for v in nx.closeness_centrality(G).values()]) print(f&#39;Correlation with in degree: {round(c_in[1,0],3)}&#39;) print(f&#39;Correlation with out degree: {round(c_out[1,0],3)}&#39;) . Correlation with in degree: 0.426 Correlation with out degree: 0.459 . From the above it can be seen that there are only some slight correlation between in- and out-degree and closeness centrality. We can see in general that it appears when a character has high degree also have high closeness centrality, but a few characters with very high degree interfer with this tendency. . 3.3.2 Eigenvector centrality . We are now going to compute the eigenvector centrality for each character in the network. The eigenvector centrality is based on the centrality of its neighbors. . sort_cent = sorted(nx.eigenvector_centrality(G).items(), key=lambda kv: kv[1], reverse = True) fig = go.Figure(data=[go.Table(header=dict(values=[&#39;&lt;b&gt;Character&lt;/b&gt;&#39;, &#39;&lt;b&gt;Centrality score (eigenvector)&lt;/b&gt;&#39;]), cells=dict(values=[ [name[0].replace(&#39;_&#39;, &#39; &#39;) for name in sort_cent], [np.round(name[1],3) for name in sort_cent] ])) ]) fig.update_layout( height=310, showlegend=False, title_text = &quot;Eigenvector centrality score of each character&quot; ) fig.show() . . . Again, we notice that main characters has the highest eigenvector centrality score, and almost the same characters are in top 5. Further, the largest eigenvector centrality is 0.228 which is Jon Snow. . Again, we will visualize the centrality vs. the in and out-degree to see if there are any correlation. . plt.figure(figsize = (12,8)) plt.scatter([e for n, e in top5_in_degrees],nx.eigenvector_centrality(G).values(), color= &#39;red&#39;, label = &#39;In degree&#39;) plt.scatter([e for n, e in top5_out_degrees],nx.eigenvector_centrality(G).values(), color= &#39;blue&#39;, label = &#39;Out degree&#39;) plt.title(&#39;Eigenvector centrality vs. degree&#39;, fontsize = 20) plt.xlabel(&#39;Degree&#39;, fontsize = 12) plt.ylabel(&#39;Eigenvector centrality&#39;, fontsize = 12) plt.legend() plt.show() c_in = np.corrcoef([d for name, d in top5_in_degrees],[v for v in nx.eigenvector_centrality(G).values()]) c_out = np.corrcoef([d for name, d in top5_out_degrees],[v for v in nx.eigenvector_centrality(G).values()]) print(f&#39;Correlation with in degree: {round(c_in[1,0],3)}&#39;) print(f&#39;Correlation with out degree: {round(c_out[1,0],3)}&#39;) . Correlation with in degree: 0.414 Correlation with out degree: 0.438 . The result is very similar to the one achieved by using the closeness centrality measure. Again we see some correlation between the degree and the eigenvector centrality. When the degree is increasing the variation in eigenvector centrality appears to increase as well. . 3.3.4 Betweenness centrality . As a last metric of centrality we will investigate the betweenness centrality, which is estimated as the sum of the fraction of all shortest paths that passes through the given node. . sort_cent = sorted(nx.betweenness_centrality(G).items(), key=lambda kv: kv[1], reverse = True) fig = go.Figure(data=[go.Table(header=dict(values=[&#39;&lt;b&gt;Character&lt;/b&gt;&#39;, &#39;&lt;b&gt;Centrality score (betweenness)&lt;/b&gt;&#39;]), cells=dict(values=[ [name[0].replace(&#39;_&#39;, &#39; &#39;) for name in sort_cent], [np.round(name[1],3) for name in sort_cent] ])) ]) fig.update_layout( height=310, showlegend=False, title_text = &quot;Betweenness centrality score of each character&quot; ) fig.show() . . . Again, the characters in top 5 are almost the same, and the highest betweenness centrality is 0.11 assigned to Jon Snow. We are again going to investigate the correlation between the betweenness centrality and degree (in and out). . plt.figure(figsize = (12,8)) plt.scatter([e for n, e in top5_in_degrees],nx.betweenness_centrality(G).values(), color= &#39;red&#39;, label = &#39;In degree&#39;) plt.scatter([e for n, e in top5_out_degrees],nx.betweenness_centrality(G).values(), color= &#39;blue&#39;, label = &#39;Out degree&#39;) plt.title(&#39;Betweenness centrality vs. degree&#39;, fontsize = 20) plt.xlabel(&#39;Degree&#39;, fontsize = 12) plt.ylabel(&#39;Betweenness centrality&#39;, fontsize = 12) plt.legend() plt.show() c_in = np.corrcoef([d for name, d in top5_in_degrees],[v for v in nx.betweenness_centrality(G).values()]) c_out = np.corrcoef([d for name, d in top5_out_degrees],[v for v in nx.betweenness_centrality(G).values()]) print(f&#39;Correlation with in degree: {round(c_in[1,0],3)}&#39;) print(f&#39;Correlation with out degree: {round(c_out[1,0],3)}&#39;) . Correlation with in degree: 0.379 Correlation with out degree: 0.391 . We can see that the centrality score in general are lower, and a few points are pulling the tendency up whereas most are quite low. The corerlation coefficients are again quite low only between 0.38-0.39. . 3.4 VoteRank . As a last property of the network we are going to investigate the VoteRank of the network. We are going to use the VoteRank algorithm to find a number of important nodes. The VoteRank algorithm computes a ranking for all nodes in the network in an iterative manner, where each node vote for it in-neighbors and the node with the laregst amount of votes are selected. The selected nodes effect on the voting are decreased iteratively, so they dont keep influencing the algorithm. . We are going to find the top 5 characters based on the VoteRank algorithm using the voterank function. . ranked = [name.replace(&#39;_&#39;,&#39; &#39;) for name in nx.voterank(G,5)] print(f&#39;Top 5 most important characters based on the VoteRank algorithm: n {ranked}&#39;) . Top 5 most important characters based on the VoteRank algorithm: [&#39;Jon Snow&#39;, &#39;Sansa Stark&#39;, &#39;Tyrion Lannister&#39;, &#39;Arya Stark&#39;, &#39;Jaime Lannister&#39;] . Again, Jon Snow appear to be the most important character. The other characters are also main characters from the series, so it would make sense they are part of being the most important. . 4. Important characters for each season . This section is devoted to explain how we did create our interactive visualization of the character degree and centrality measures. The app can be found by following this link. As Heroku only makes it possible to host one app pr. Github repository the code are uploaded to a separate Github repository, which can be found here. . The code can be found in this python file. The app contain three main parts: the season tabs which is created using dash_core_components.Tabs, the table which is create using dash_table.DataTable, where it has been selected to include functionalities such as deleting columns and rows, sorting and filtering the data. The last component is the barplots below the table which also responds to sorting, filtering and deletions. . The idea with the table is to make it possible for the end user to go through the seasons and investigate what characters are most connected and have the highest centrality score. Instead of listing 8 tables we thought this were a more engaging and nice way to do it. . In order to generate the needed data, we did iterate through all seasons and loaded the gpickle file for the given season, computed closeness centrality, in- and out-degree for each character and saved it in a dataframe. . All the dataframes were saved in a dictionary which were saved in a pickle that could be loaded by the app. The code for generating thepickle file can be seen below. . tables = {} #Iterate through each season: for i in range(1,9): #Load network G = nx.read_gpickle(&quot;../data/got_G_s&quot;+str(i)+&quot;.gpickle&quot;) #Compute out centrality: centrality = nx.closeness_centrality(G) #Store in, out, centrality and character name in list of lists att = [[n.replace(&#39;_&#39;,&#39; &#39;),G.in_degree(n), G.out_degree(n), np.round(centrality[n],3)] for n, d in G.nodes(data = True)] #Save to dataframe and rename columns df = pd.DataFrame(att) df.columns = [&#39;Character name&#39;, &#39;In degree&#39;, &#39;Out degree&#39;, &#39;Closeness centrality&#39;] #Save in dictionary: tables[i] = df import pickle #Save pickle file with open(&#39;tables.pickle&#39;, &#39;wb&#39;) as handle: pickle.dump(tables, handle, protocol=pickle.HIGHEST_PROTOCOL) . The running app can be seen below: . From the table it can be seen that the most connected and maybe important characters in season 1 might be Eddard Stark and Robert Baratheon, which makes perfect sense as Eddard is present a lot in season 1, both in Winterfell but also when he becomes the Kings hand. Robert is also very central in season 1, as he rules as King and dies when hunting - killed by a pig, as he says it. . Season 2, here Joffrey Baratheon, the new king, rules, and Robb Stark goes to war as he wants revenge for his fathers execution in season 1. . Later, in season 7 Jon Snow and Daenerys Targaryen becomes key players as the winter is approaching and the focus moves from Kings Landing to the wall and the fight against the dead. . 5. Subconclusion . The resulting graph across all seasons were a very dense graph, and through different network analysis tools and by splitting into seasons it was possible to determine the most important characters in each season. Further, we found that the character attributes selected (religion, culture, allegiance, appearance and status) did not pose to be good measures to distinguish the characters. The characters were possibly connected base on another attribute or underlying pattern. . In the next sections we are going to investigate community analysis and hopefully this can help us reveal some of these patterns. .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/Explainer_Network/",
          "relUrl": "/Explainer_Network/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "Explainer Notebook - Text Analysis",
          "content": "Text Analysis . In this notebook we are going to dive fully into the text analysis part of this project. This includes implementation of Term Frequency-Inverse Document Frequency (TF-IDF) and it&#39;s application to generate wordclouds. Additionally, we will also dive into two methods of sentiment analysis. Namely, the dictionary-based approached LabMT and the dictionary- and rule-based approach VADER. These approaches will be utilized and compared when doing sentiment analysis on the characters wiki-pages and dialogue in the show. Finally, we will compute a lexical dispersion plot in order to gain insights in whether the theme of the story changes throughout the show. . How the data is extracted and pre-processed is described in section 2.1.1 Data extraction, cleaning and preprocessing in the Explainer Notebook. . We will begin by importing the nescarry packages and loading in the data. . import numpy as np import networkx as nx import matplotlib.pyplot as plt import urllib import json import re import os import string import itertools import pandas as pd from wordcloud import WordCloud from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer import requests import nltk from nltk import FreqDist from nltk.text import TextCollection from nltk.tokenize import word_tokenize from nltk.tokenize import wordpunct_tokenize from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords from nltk.draw.dispersion import dispersion_plot from matplotlib import pylab #Download NLTK stuff for lemmatizer and tokenizer: nltk.download(&#39;punkt&#39;) nltk.download(&#39;wordnet&#39;) nltk.download(&quot;stopwords&quot;) # For extracting and showing web images from bs4 import BeautifulSoup import urllib from skimage import io #Set stuff for nice formmatting of plots: import seaborn as sns sns.set() %matplotlib inline # Include character names in list of stopwords char_list = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] char_names = [] for char in char_list: char = char.lower() char_names.append(char.replace(&quot;_&quot;,&quot; &quot;)) char_names.extend(char.split(&#39;_&#39;)) stop_words = set(stopwords.words(&#39;english&#39;) + list(string.punctuation) + list(char_names)) . /shared-libs/python3.7/py/lib/python3.7/site-packages/nltk/draw/__init__.py:15: UserWarning: nltk.draw package not loaded (please install Tkinter library). warnings.warn(&#34;nltk.draw package not loaded (please install Tkinter library).&#34;) [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Unzipping corpora/wordnet.zip. [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. . Before loading in the text data from the character wiki-pages, we define a helper function to do some minor data-preprocessing for computation of TF-IDF. First, all text is set to lower case, then the text is tokenized, stop words are removed from the tokenized text and finally the text is lemmatized, i.e. different inflections of the same word are grouped together to a single word. Here, it should be noted that we have included the character first and last names in the stopwords list as these do not relay any interesting information about characters, allegiance or sentiment. . def clean_text(txt): txt = txt.lower() word_tokens = wordpunct_tokenize(txt) filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] wnlet = WordNetLemmatizer() words = [wnlet.lemmatize(w) for w in filtered_sentence] return words . The dialogue data is loaded in: . resp = requests.get(&quot;https://raw.githubusercontent.com/jeffreylancaster/game-of-thrones/master/data/script-bag-of-words.json&quot;) diag = json.loads(resp.text) char_diag = {} for element in diag: episode = element[&#39;episodeNum&#39;] season = element[&#39;seasonNum&#39;] title = element[&#39;episodeTitle&#39;] text = element[&#39;text&#39;] for textObj in text: if textObj[&#39;name&#39;] in char_diag: char_diag[textObj[&#39;name&#39;]].append(textObj[&#39;text&#39;]) else: char_diag[textObj[&#39;name&#39;]] = [textObj[&#39;text&#39;]] . The wiki page text data is loaded into a dictionary with all characters as dictionary keys. The text data is first preprocessed by the helper function clean_text. . char_pages = {} characters = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] for char in characters: name = char.replace(&#39;_&#39;, &#39; &#39;) with open(&#39;../data/got_cleaned/&#39;+char+&quot;.txt&quot;, &quot;r&quot;, encoding = &quot;utf-8&quot;) as text_file: txt = text_file.readlines() txt = clean_text(&quot;&quot;.join(txt)) char_pages[name] = txt . 1. Wordclouds&lt;/p&gt; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 1.1 Term Frequency-Inverse Document Frequency . We will begin by defining the term frequency term for a given word: Number of times the word occours in the text divided by total number of words in the text. For the inverse document frequency term: $ text{log} left( dfrac{N}{df_i} right)$ where N is the number of documents and $df_i$ is the document frequency of a given word. In our case, we want to compute the TC-IDF for four different sets of documents; namely, for the dialogue, the characters, the allegiances, and for a given season in the show. . We will start by computing the TF-IDF for the character dialogues. This is done by first computing the term count of every character document, i.e. dialogue, and in order to convert these term counts to frequencies later on, the total word count of the document as well. This step can be seen below. . #Create space for dict: tc_dict_char = {} l_dict_char = {} # List of characters char_list = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] # list of all characters # Iterate through all characters for char_key in char_list: try: words = char_diag[char_key.replace(&quot;_&quot;, &quot; &quot;)] #extract dialogue for given character words = &quot; &quot;.join(words) except: continue words = clean_text(words) #Total word count for given character l_dict_char[char_key] = len(words) #Compute TC for the tokens in character document tc_dict_char[char_key] = FreqDist(words) . Now, the IDF can be computed. In order to compute the document frequency of a given word, $df_i$, we first find all unique words in all character documents. Then, using the formula stated above for IDF, we compute the IDF. In this case, we have 137 unique documents, N, and simply count, for every unique word, in how many of these documents a given word occour. . unique_words = set(list(itertools.chain(*tc_dict_char.values()))) # Calculate idf idf = {word: #Find in how many documents that each word is present: np.log(len(tc_dict_char) / sum([tf[word] &gt; 0 for k, tf in tc_dict_char.items()])) #Iterate through unique words: for word in unique_words } . Finally, we can compute the TF-IDF for the character dialogue as seen below. In order to convert the before computed term counts to term frequencies, we simply divide the TC with the computed total word count of the given document as well. . tf_idf_char = {} #Iterate through communities: for char in tc_dict_char.keys(): tf_idf = {} #Iterate through each word in each community: for word, tc_value in tc_dict_char[char].items(): #Extract IDF idf_value = idf[word] #Compute TF-IDF tf_idf[word] = idf_value*tc_value/l_dict_char[char] tf_idf_char[char] = tf_idf . The same approach is taken when computing the TF-IDF for the character pages. In this case we have 163 documents. . Now, for the third set of documents we want to investigate, namely for the different allegiances, we first have to compile documents for all the different allegiances. When we created the overall Game of Thrones network, one of the saved attributes were allegiance. This is therefore utilized in order to compile this set of documents. All character documents from the dialogue of the same allegiance is pooled into one single document for the given allegiance. Otherwise, the approach is the same of the previous. . # Load in network G = nx.read_gpickle(&quot;../data/got_G.gpickle&quot;) # List of allegiances for all characters allegiances = list([allegiance for char, allegiance in nx.get_node_attributes(G, &quot;allegiance&quot;).items() if allegiance != &quot;&quot;]) # Value count of the different allegiances allegiances = [[allegiances.count(att),att]for att in set(allegiances) if att != &quot;&quot;] # Select 6 largest allegiances chosen = sorted(allegiances, key = lambda i: i[0], reverse = True)[:6] # List of all unique allegiances allegiances = list(set([allegiance for char, allegiance in nx.get_node_attributes(G, &quot;allegiance&quot;).items() if allegiance != &quot;&quot;])) l_dict_al, tc_dict_al = {}, {} # initialize dicts for allegiance in allegiances: # iterate through the allegiances words = [] for char, a2 in nx.get_node_attributes(G, &quot;allegiance&quot;).items(): # Iterate through the characters and their respective allegiance if a2 == allegiance: try: words.extend(char_diag[char.replace(&quot;_&quot;, &quot; &quot;)]) except: continue words = clean_text(&quot; &quot;.join(words)) #Total word count for given character l_dict_al[allegiance] = len(words) #Compute TC for the community words: tc_dict_al[allegiance] = FreqDist(words) . And the same approach is then taken as previous when computing the TF-IDF. . unique_words = set(list(itertools.chain(*tc_dict_al.values()))) # Calculate idf idf_al = {word: #Find in how many documents that each word is present: np.log(len(allegiances) / sum([tf[word] &gt; 0 for k, tf in tc_dict_al.items()])) #Iterate through unique words: for word in unique_words } #Create dict for tf-idf values: tf_idf_al = {} #Iterate through communities: for allegiance in allegiances: tf_idf = {} #Iterate through each word in each community: for word, tf_value in tc_dict_al[allegiance].items(): #Extract IDF idf_value = idf_al[word] #Compute TF-IDF tf_idf[word] = idf_value*tf_value/l_dict_al[allegiance] tf_idf_al[allegiance] = tf_idf . Finally, the TF-IDF will now be computed for the last set of documents, namely the seasons. Here, we pool the documents of all characters present in a given season. . char_season_wiki = {} base_path = &quot;../data/got2/s&quot; for s in range(1,9): txt = [] files = os.listdir(base_path + str(s)+&quot;_cleaned/&quot;) for file in files: with open(&#39;../data/got2/s&#39;+str(s)+&quot;_cleaned/&quot;+ file, &quot;r&quot;, encoding = &#39;utf-8&#39;) as text_file: tmp = text_file.readlines() txt.extend(tmp) char_season_wiki[&quot;s&quot;+str(s)] = clean_text(&quot; &quot;.join(txt)) tf_idf_season = tf_idf_func(char_season_wiki,[&#39;s&#39;+str(i) for i in range(1,9)]) . 1.2 Character Wordclouds . As we now have three datasets with computed TF-IDF scores, we need a good way to present the results, and we have chosen to use Wordclouds as a way to present the TF-IDF scores. The words are then scaled in size based on the TF-IDF score, and by that we can easily see the frequent used words, and try to see if we can understand the characters and allegiances better by investigating these results. . In the first part we will look at the character wordclouds and compare these between the character dialogoue and the wiki-pages. To set the scene we have also included the character image. . We first define two functions which is used to retrieve the image from the character page. We are using BeautifulSoup to scrape the character wiki-page for the thumbnail image. . def getdata(url): &quot;&quot;&quot; Function retrieves the data on the character page &quot;&quot;&quot; r = requests.get(url) return r.text def get_img(name): &quot;&quot;&quot; Uses the retrieved information to find the thumbnail image, by scraping the charater page &quot;&quot;&quot; htmldata = getdata(&quot;https://gameofthrones.fandom.com/wiki/&quot;+name) soup = BeautifulSoup(htmldata, &#39;html.parser&#39;) image = soup.find(&#39;img&#39;, attrs={&quot;class&quot;:&quot;pi-image-thumbnail&quot;}) return image[&#39;src&#39;] . Next, we are setting up af function to create the visualization displaying both the character image, Wordclouds based on character dialogoue and character wiki-page. Which can be seen below. We have updated some of the stats for the wordclouds ie. the color maps, sizes to make them more appealing. . def plot_wordcloud_characters(selected_char, tf_idf_char_pages, tf_idf_diag): &quot;&quot;&quot; Function to display wordclouds of selected characters Displays the character image, wordclouds based on dialogoue and character wiki-page &quot;&quot;&quot; #Setup figure: plt.figure(figsize=(20,25)) #Set colours color_lst = [&#39;Blues&#39;,&#39;Purples&#39;,&#39;autumn&#39;,&#39;rocket&#39;,&#39;spring_r&#39;] #The figure has three index: 1. image, 2. wordcloud (Wiki-page) 3. wordlcoud (dialogoue) plot_idx = 1 i = 0 #Iterate through characters for char in selected_char: word_list = [] #Extract words for wiki-page for word, value in tf_idf_char_pages[char].items(): word_list.append((word+&#39; &#39;)*int(np.ceil(value))) word_list = &quot; &quot;.join(word_list) #Generate wordcloud wc_char_pages = WordCloud(collocations = False, background_color=&#39;black&#39;, colormap=color_lst[i], width = 400, height = 450).generate(word_list) word_list = [] #Do the same for dialogoue for word, value in tf_idf_diag[char].items(): word_list.append((word+&#39; &#39;)*int(np.ceil(value))) word_list = &quot; &quot;.join(word_list) wc_char_diag = WordCloud(collocations = False,background_color=&#39;black&#39;, colormap=color_lst[i], width = 400, height = 450).generate(word_list) #Visualize figure: plt.subplot(5,3,plot_idx) url = get_img(char) img = io.imread(url) plt.imshow(img, extent = [0,5,0,6], aspect = 1) plt.axis(&quot;off&quot;) plot_idx += 1 plt.subplot(5,3,plot_idx) plt.imshow(wc_char_pages, interpolation=&#39;bilinear&#39;)#extent = [0,6,0,6], aspect = 1 if plot_idx == 2: plt.title(&quot;From Wiki Page&quot;, fontsize = 40, color = &#39;black&#39;) plt.axis(&quot;off&quot;) plot_idx += 1 plt.subplot(5,3,plot_idx) plt.imshow(wc_char_diag, interpolation=&#39;bilinear&#39;)#, extent = [0,6,0,6], aspect = 1 if plot_idx == 3: plt.title(&quot;From Character nDialogoue&quot;, fontsize = 40, color = &#39;black&#39;) plt.axis(&quot;off&quot;) plot_idx += 1 i += 1 plt.margins(x=0, y = 0) plt.tight_layout() plt.show() . We have selected the characters: Jon Snow, Arya Stark, Bronn, Brienne of Tarth and Jaime Lannister, and we are going to look at these wordclouds. The wordclouds are generated using the builtin function WordCloud, which makes it easy to create nice visualizations. . selected_characters = [&#39;Jon_Snow&#39;,&#39;Arya_Stark&#39;,&#39;Bronn&#39;,&#39;Brienne_of_Tarth&#39;,&#39;Jaime_Lannister&#39;] plot_wordcloud_characters(selected_characters,tf_idf_char_page,tf_idf_char) . If we look at Bronn we can see for the wiki-page that the word with highest score is sellssword, and follow with respect to the dialogoue, which are very well describing words of him. He follows Tyrion Lannister, and is a sellsword. If we look at Brienne of Tarth, she becomes part of Renly Baratheons Kingsguard, and she swears to protect multiple persons in the story including Renly and Catelyn. . When comparing the generated wordclouds for the respective data sets it should be noted, that the same words are, for the most part, not present for the respective characters. This is expected as one would imagine that the text from the characters wikipedia pages are more descriptive of the character and their place in the story whereas the wordcloud from the dialogoue is exactly that; their most descrriptive words according to TF-IDC used throughout the series. This would be interesting to compare with sentiment analysis which is the second part of this page. . 1.3 Allegiance Wordclouds . As described above we have pooled the TF-IDF scores for the characters present in a allegiance. For this, we have selected the houses: Stark, Lannister, Targaryen, Greyjoy and the independent group The Night&#39;s Watch. It would be interesting to see, if the houses mottos would appear in these word clouds. The respective house mottos are: . House Stark: Winter is coming House Lannister: Hear Me Roar! House Targaryen: Fire and Blood House Greyjoy: We Do Not Sow . As the Night&#39;s Watch is not a House but rather a brotherhood sworn to protect The Wall, they do not have a motto. . We have created a function to plot the mottos, again using BeautifulSoup to extract the thumbnail images of the house logos, and WordCloud function to create the wordclouds. The result can be seen below. . def plot_wordcloud_allegiance(selected_al, tf_idf_al): plt.figure(figsize=(24,28)) color_lst = [&#39;Blues&#39;,&#39;autumn&#39;,&#39;inferno&#39;,&#39;magma_r&#39;,&#39;Greys&#39;] i = 0 plot_idx = 1 for allegiance in selected_al: word_list = [] for word, value in tf_idf_al[allegiance].items(): word_list.append((word+&#39; &#39;)*int(np.ceil(value))) word_list = &quot; &quot;.join(word_list) wc_char_al = WordCloud(collocations = False, background_color=&#39;black&#39;, colormap=color_lst[i]).generate(word_list) plt.subplot(6,2,plot_idx) url = get_img(allegiance.replace(&quot; &quot;,&quot;_&quot;)) img = io.imread(url) plt.imshow(img, extent = [0,5,0,6], aspect = 1) plt.axis(&quot;off&quot;) plt.title(allegiance, fontsize = 40) plot_idx += 1 plt.subplot(6,2,plot_idx) plt.imshow(wc_char_al, interpolation=&#39;bilinear&#39;)#,extent = [0,6,0,6], aspect = 1) plt.axis(&quot;off&quot;) plot_idx += 1 i += 1 plt.tight_layout() plt.show() selected_al = [&#39;House Stark&#39;,&#39;House Lannister&#39;,&#39;House Targaryen&#39;,&#39;House Greyjoy&#39;,&#39;Night &#39;s Watch&#39;] plot_wordcloud_allegiance(selected_al, tf_idf_al) . When looking at the wordclouds above and the respective house mottos, only the Lannisters&#39; Hear (big, middle) are present. All the wordclouds are, however, very descriptive of the respective houses. For instance for the Night&#39;s Watch, a military order sworn to protect The Wall, words like protect, wildling and swear are present. The same can be said for House Targaryan, where the main Targaryan character, Daenerys, is married to a dothraki warlord and later in the show, is a leader of dothraki people herself. . 1.4 Season Wordclouds . One of our goal insights that we want to investigate in this text analysis section is to understand whether the overall theme of the Game Of Thrones series changes season by season. One way we want to investigate this is by looking at the wordclouds for each season, again with word size based on the TF-IDF score. By this we can get indication of the important words in the given season and hopefully understand what is the overarching theme in each season. We have again created a function to plot these wordclouds again utilzing the WordCloud functionality. . def plot_wordcloud_season(seasons,tc_idf): plt.figure(figsize = (12,15)) for i,season in enumerate(seasons): word_list= [] for word, value in tc_idf[season].items(): word_list.append((word+&#39; &#39;)*int(np.ceil(value))) word_list = &quot; &quot;.join(word_list) wc = WordCloud(collocations=False, background_color=&#39;black&#39;, colormap=&#39;autumn_r&#39;).generate(word_list) plt.subplot(4,2,i+1) plt.imshow(wc, interpolation=&#39;bilinear&#39;) title = &quot;Season {}&quot;.format(i+1) plt.title(title, fontsize = 40) plt.axis(&quot;off&quot;) plt.tight_layout() plt.show() plot_wordcloud_season([&quot;s&quot;+ str(i) for i in range(1,9)], tf_idf_season) . Taking example in the wordclouds generated for season 1 &amp; 8, the emphasized words seem very descriptive of their respective seasons. Starting with season 1: . execute, behead : One of the main acts of season 1, is the execution of Lord Eddard Stark, the head of House Stark. He is, by the unexpected command of the king Joffrey Baratheon, beheaded in the middle of King&#39;s Landing. | Khal, bloodrider : Another of the main story arcs, is the story of Daenarys Targaryan which takes place in a foreign land. In season 1, Daenarys is married of to a powerful Khal, Khal Drogo, in a trade by Daenarys brother. A Khal has three bloodriders who are to live and die by the life of their Khal. The words Khal and bloodrider being so prominent makes sense, as they are key roles in Daenarys&#39; story arc. | . Comparing the wordclouds of season 1 and season 8, it appears season 8 has different key words. For season 8: . celebrate : The word celebrate stands in stark constrast to the prominent words suffer from season 1. This could be due to season 8 being the series final season and it&#39;s characters are therefore celebrating the story ending on a happy note (for some of the characters :wink: ) | reunite : The story culminates in the final season, many characters who have been seperated throughout the show are finally reunited in the final season of the show, hence emphasis on the word reunite makes sense. | . It should also be noted that the word destroy is present in the majority of the wordclouds, only being omitted in the wordclouds for season 1 and 3. . 2. Sentiment Analysis . In this part of the text analysis, we will now do sentiment analysis. This will be done on both the characters wiki-pages but also their dialogue. Additionally, sentiment analysis will also be done across the different seasons of the Game of Thrones. . 2.1 Dictionary- and Rule-based sentiment analysis . For sentiment analysis in this project, we will make use of two methods: namely, LabMT and VADER. Both of these methods utilize a dictionary-based method. Simply put, both approaches utilizes a dictionary of sentiment laden words. The sentiment laden words have been given a score depending on the judged sentiment of the word. VADER imposes an additional method: rule-based. These rules were originally implemented to help asses the sentiment of sentences in social media. The rules help convey how degree adverbs can change the sentiment. Furthermore, VADER has also imposed rules to convey how punctuation and capitalization can change the sentiment of a given sentence. Due to VADER also taking degree adverbs, capitalzation and punctuation into account, this method expects whole sentences and stopwords havent been removed compared to LabMT which simply expects tokens. It should also be noted that the two methods score sentiment on a different scale. LabMT scores on a scale from 1 to 9 while VADER scores from -4 to 4. A score of 5 is considered neutral for LabMT while for VADER neutral is between -0.05 and 0.05. . . We will begin the sentiment analysis by defining some helper functions to utilize the LabMT and VADER sentiment analysers. . LabMT = pd.read_table(&#39;../data/labMIT-1.0.txt&#39;, delimiter=&quot; t&quot;) #Convert LabMT to a dictionary: LabMT_dict = {word : happiness_score for word,happiness_score in zip(LabMT[&#39;word&#39;], LabMT[&#39;happiness_average&#39;]) } #Load VADER wordlist: analyzer = SentimentIntensityAnalyzer() #Function to compute LabMT sentiment values of tokens: lemmatizer = WordNetLemmatizer() def sentiment_LabMT(tokens): #Extract tokens that are present in LabMT: tokens_LabMT = [token for token in tokens if token in LabMT_dict.keys()] #Extract sentiment values of tokens: happiness_LabMT = [LabMT_dict[token] for token in tokens_LabMT] #Return mean values of sentiments for given tokens: return np.mean(happiness_LabMT) #Function to copmpute VADER sentiment values of a sentence: def sentiment_VADER(tokens): happiness_VADER = [analyzer.polarity_scores(sentence)[&#39;compound&#39;] for sentence in tokens] return np.mean(happiness_VADER) . And a function to plot the sentiment analysis results. . import plotly.graph_objects as go from plotly.subplots import make_subplots def plot_VADER_LabMT_scores(char_sentiment_VADER,char_sentiment_LabMT, title,error_bar = False, com_sentiment_VADER_sd=None, com_sentiment_LabMT_sd=None, x_text = &quot;Characters&quot; ): # Create figure fig = make_subplots(rows=1, cols=2) # Add traces if error_bar: fig.add_trace( go.Bar(x=[*char_sentiment_LabMT], y=list(char_sentiment_LabMT.values()), error_y=dict( type=&#39;data&#39;, # value of error bar given in data coordinates array= list(com_sentiment_LabMT_sd.values()), visible=True),name=&quot;LabMT score&quot;), row=1, col=1, ) fig.add_trace( go.Bar(x=[*char_sentiment_VADER], y=list(char_sentiment_VADER.values()), error_y=dict( type=&#39;data&#39;, # value of error bar given in data coordinates array= list(com_sentiment_VADER_sd.values()), visible=True), name=&quot;VADER score&quot;), row=1, col=2, ) else: fig.add_trace( go.Bar(x=[*char_sentiment_LabMT], y=list(char_sentiment_LabMT.values()), name=&quot;LabMT score&quot;), row=1, col=1, ) fig.add_trace( go.Bar(x=[*char_sentiment_VADER], y=list(char_sentiment_VADER.values()), name=&quot;VADER score&quot;), row=1, col=2, ) # Add figure title fig.update_layout( title_text=title ) # Set x-axis title fig.update_xaxes(title_text=x_text) # Set y-axes titles fig.update_yaxes(title_text=&quot;&lt;b&gt;LabMT sentiment score&lt;/b&gt;&quot;, row=1,col = 1) fig.update_yaxes(title_text=&quot;&lt;b&gt;VADER sentiment score&lt;/b&gt; &quot;, row = 1, col = 2) return fig . 2.2 Sentiment analysis on characters&#39; dialogue . We will now begin on the sentiment analysis. For this subsection, sentiment analysis will be done on the characters&#39; dialogue. This is based on all dialogoue across all seasons as this is expected to give a better overview of each character sentiments. We start by loading in the dialogue data and compute the sentiment values using the before defined helper functions. . tokens_LabMT = {char : clean_text(&quot; &quot;.join(text)) for char, text in char_diag.items()} tokens_VADER = char_diag files = os.listdir(&quot;../data/got/&quot;) char_list = [file.split(&#39;.txt&#39;)[0] for file in files] #Suprress warnings: warnings.filterwarnings(&quot;ignore&quot;) #Compute sentiment for each character: char_sentiment_LabMT = {char :sentiment_LabMT(tokens_values) for char, tokens_values in tokens_LabMT.items() if char.replace(&quot; &quot;, &quot;_&quot;) in char_list} char_sentiment_VADER = {char :sentiment_VADER(tokens_values) for char, tokens_values in tokens_VADER.items() if char.replace(&quot; &quot;, &quot;_&quot;) in char_list} . The found sentiment scores are then sorted in order to find which characters are the happiest and saddest. Here, we print the top 10 happiest and saddest characters. . happiest_VADER = sorted(char_sentiment_VADER, key = lambda i: char_sentiment_VADER[i],reverse = True)[:10] happiest_LabMT = sorted(char_sentiment_LabMT, key = lambda i: char_sentiment_LabMT[i],reverse = True)[:10] sadest_VADER = sorted(char_sentiment_VADER, key = lambda i: char_sentiment_VADER[i],reverse = False)[:10] sadest_LabMT = sorted(char_sentiment_LabMT, key = lambda i: char_sentiment_LabMT[i],reverse = False)[:10] print(&#39;Happiest based on VADER: &#39;,happiest_VADER) print(&#39;Happiest based on LabMT: &#39;,happiest_LabMT) print(&#39;Sadest based on VADER: &#39;,sadest_VADER) print(&#39;Sadest based on LabMT: &#39;,sadest_LabMT) . Happiest based on VADER: [&#39;Dontos Hollard&#39;, &#39;Daisy&#39;, &#39;Pyat Pree&#39;, &#39;Harry Strickland&#39;, &#39;Olyvar&#39;, &#39;Mace Tyrell&#39;, &#39;Royal Steward&#39;, &#39;Margaery Tyrell&#39;, &#39;Matthos Seaworth&#39;, &#39;Marillion&#39;] Happiest based on LabMT: [&#39;Alys Karstark&#39;, &#39;Daisy&#39;, &#39;Pyat Pree&#39;, &#39;Matthos Seaworth&#39;, &#39;Myrcella Baratheon&#39;, &#39;Lyanna Stark&#39;, &#39;Olyvar&#39;, &#39;Rhaegar Targaryen&#39;, &#39;Royal Steward&#39;, &#39;Trystane Martell&#39;] Sadest based on VADER: [&#39;Anya Waynwood&#39;, &#39;Izembaro&#39;, &#39;Rorge&#39;, &#39;Hallyne&#39;, &#39;Rickard Karstark&#39;, &#39;Gregor Clegane&#39;, &#39;Leaf&#39;, &#39;Pypar&#39;, &#39;Syrio Forel&#39;, &#39;Obara Sand&#39;] Sadest based on LabMT: [&#39;Gregor Clegane&#39;, &#39;Rakharo&#39;, &#39;Qotho&#39;, &#39;Pypar&#39;, &#39;Rickard Karstark&#39;, &#39;Grenn&#39;, &#39;Greatjon Umber&#39;, &#39;Sandor Clegane&#39;, &#39;Rast&#39;, &#39;Meryn Trant&#39;] . The figure below presents the sentiment of the 10 happiest and 10 sadest characters. To the left the sentiment are based on LabMT whereas the figure to the right is based on VADER. . It should be noted that the two methods does not completely agree, but some characters are present in both results such as: Daisy, Pyat Pree, Olyvar and Matthos Seaworth are in top 10 of the happiest character in both results. Also some characters are present in both lists presenting the sadest characters such as Gregor Clegane. . The happiest characters appear to be quite happy based on the VADER and LabMT score as the score only goes to 1 for VADER and 9 for LabMT and the same for saddest characters. . sadest_VADER.reverse() sadest_LabMT.reverse() plot_dict_vader = {key: char_sentiment_VADER[key] for key in happiest_VADER+sadest_VADER} plot_dict_LabMT = {key: char_sentiment_LabMT[key] for key in happiest_LabMT+sadest_LabMT} fig = plot_VADER_LabMT_scores(plot_dict_vader, plot_dict_LabMT, title = &quot;Sentiment analysis of character dialogoue&quot;) fig.show() . . . 2.3 Sentiment analysis on characters&#39; wiki-page . We will now do sentiment analysis on the characters&#39; wiki-pages. Similair as before, we start by loading in the text data and compute the sentiment values using the before defined helper functions. . characters = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] char_pages = {} for char in characters: name = char.replace(&#39;_&#39;, &#39; &#39;) with open(&#39;../data/got_cleaned/&#39;+char+&quot;.txt&quot;, &quot;r&quot;, encoding = &quot;utf-8&quot;) as text_file: txt = text_file.readlines() char_pages[name] = txt tokens_LabMT = {char : clean_text(&quot; &quot;.join(text)) for char, text in char_pages.items()} tokens_VADER = char_pages #Suprress warnings: warnings.filterwarnings(&quot;ignore&quot;) #Compute sentiment for each character with use of the before defined helper functions: char_sentiment_LabMT = {char :sentiment_LabMT(tokens_values) for char, tokens_values in tokens_LabMT.items() } char_sentiment_VADER = {char :sentiment_VADER(tokens_values) for char, tokens_values in tokens_VADER.items() } . Again, we sort the results and find the top 10 happiest and saddest characters. . happiest_VADER = sorted(char_sentiment_VADER, key = lambda i: char_sentiment_VADER[i],reverse = True)[:10] happiest_LabMT = sorted(char_sentiment_LabMT, key = lambda i: char_sentiment_LabMT[i],reverse = True)[:10] sadest_VADER = sorted(char_sentiment_VADER, key = lambda i: char_sentiment_VADER[i],reverse = False)[:10] sadest_LabMT = sorted(char_sentiment_LabMT, key = lambda i: char_sentiment_LabMT[i],reverse = False)[:10] print(&#39;Happiest based on VADER: &#39;,happiest_VADER) print(&#39;Happiest based on LabMT: &#39;,happiest_LabMT) print(&#39;Sadest based on VADER: &#39;,sadest_VADER) print(&#39;Sadest based on LabMT: &#39;,sadest_LabMT) . Happiest based on VADER: [&#39;Kraznys mo Nakloz&#39;, &#39;Xaro Xhoan Daxos&#39;, &#39;Leaf&#39;, &#39;Septa&#39;, &#39;Moro&#39;, &#39;Missandei&#39;, &#39;Anguy&#39;, &#39;Lyanna Stark&#39;, &#39;Renly Baratheon&#39;, &#39;Matthos Seaworth&#39;] Happiest based on LabMT: [&#39;Septa&#39;, &#39;Order of Maesters&#39;, &#39;Moro&#39;, &#39;Matthos Seaworth&#39;, &#39;Leaf&#39;, &#39;Margaery Tyrell&#39;, &#39;Selyse Baratheon&#39;, &#39;Marillion&#39;, &#39;Tycho Nestoris&#39;, &#39;Ros&#39;] Sadest based on VADER: [&#39;Amory Lorch&#39;, &#39;Orell&#39;, &#39;Karl Tanner&#39;, &#39;Polliver&#39;, &#39;Rast&#39;, &#39;Ramsay Bolton&#39;, &#39;Joffrey Baratheon&#39;, &#39;Alliser Thorne&#39;, &#39;Randyll Tarly&#39;, &#39;Obara Sand&#39;] Sadest based on LabMT: [&#39;Biter&#39;, &#39;Amory Lorch&#39;, &#39;Rorge&#39;, &#39;Polliver&#39;, &#39;Lommy&#39;, &#39;Orell&#39;, &#39;Qhono&#39;, &#39;Rast&#39;, &#39;Black Lorren&#39;, &#39;Gregor Clegane&#39;] . From the figure below it can be seen that the two methods, again, do not completely agree on the result but both methods yield approximately the same result. Again the figure displays the 10 happiest and sadest characters based on LabMT and VADER. . At a first glance, it is noticed that the VADER score are lower for the happiest characters than in the previous part whereas the sadest achieve almost the same score. The LabMT results are quite similar in sentiment levels. Again many characters are found in both results such as Septa, Moro, Orell and Polliver. . When comparing with the result based on the character dialogoue not many characters are found in all four results. This could indicate that the wiki-pages and dialogoue does not contain the same information, or that the chosen words on the wiki-pages do not necessarily imply information about the characters sentiment. . It would be expected that the dialogoue contains greater variety of words that can explain the character mood, whereas the wiki-pages would contain words that describe the character and his/hers actions. We also notice that the variation in VADER sentiment scores are far greater when using the dialogoue compared with the wiki-page which could be an indication that our hypothesis are true. . sadest_VADER.reverse() sadest_LabMT.reverse() plot_dict_vader = {key: char_sentiment_VADER[key] for key in happiest_VADER+sadest_VADER} plot_dict_LabMT = {key: char_sentiment_LabMT[key] for key in happiest_LabMT+sadest_LabMT} fig = plot_VADER_LabMT_scores(plot_dict_vader, plot_dict_LabMT, title =&quot;Sentiment analysis of character wiki-page&quot; ) fig.show() . . . 2.4 Sentiment analysis on the series&#39; seasons . As a last element in our sentiment analysis we are going to dive into the sentiment of each season. This could help us investigate whether the general mode changes in each season. Using same approach as before, we start by loading in the data. . char_season_wiki = {} base_path = &quot;../data/got2/s&quot; for s in range(1,9): txt = [] files = os.listdir(base_path + str(s)+&quot;_cleaned/&quot;) for file in files: with open(&#39;../data/got2/s&#39;+str(s)+&quot;_cleaned/&quot;+ file, &quot;r&quot;, encoding = &quot;utf-8&quot;) as text_file: tmp = text_file.readlines() txt.extend(tmp) char_season_wiki[&quot;s&quot;+str(s)] = clean_text( &quot; &quot;.join(txt)) . The sentiment scores are then computed. . tokens_LabMT = {char : text for char, text in char_season_wiki.items()} tokens_VADER = char_season_wiki #Suprress warnings: warnings.filterwarnings(&quot;ignore&quot;) #Compute sentiment for each character: char_sentiment_LabMT = {char :sentiment_LabMT(tokens_values) for char, tokens_values in tokens_LabMT.items() } char_sentiment_VADER = {char :sentiment_VADER(tokens_values) for char, tokens_values in tokens_VADER.items() } . The figure below shows the sentiment of each season based on LabMT and VADER methods. When looking at the LabMT it can be seen that all season are approximately neutral, whereas the VADER scores are just to the sad side of the spectrum. Further, it is noticed that season 4 are the sadest whereas season 6 are the &quot;happiest&quot; when comparing them. . In season 4 a lot of the semi-main characters die such as Prince Oberyn, Joffrey Baratheon, Shay, Tywin Lannister and the Mountain (Gregor Clegane) are transformed into the Monster version of himself. Which could explain why this season is saddest according to the sentiment analysis. . fig = plot_VADER_LabMT_scores(char_sentiment_VADER, char_sentiment_LabMT, title = &quot;Sentiment analysis of seasons&quot;,x_text = &#39;Season&#39;) fig.show() . . . 3. Lexical Dispersion Plot . We have already attempted to investigate how the theme changes season by season in Game Of Thrones by investigating the wordclouds for each season. As we want to investigate this a little further, we have also looked into lexical dispersion plot. Here we carefully selected a few words, which we expect to help us dive further into the changing theme of the series. Through the use of the lexical dispersion we expect to see how the theme changes by the change in frequency of the selected words season by season. . First, we to set up the data in the correct order. We are again using the character dialogoue data for this, as we want to investigate the words that the characters use and thereby the theme in the series changes. We are going to tokenize the words, remove stopwords and lemmatize the words before saving them in a dictionary. The dictionary contains the season number as keys and values are the words used in the season. . season_words = {} #Set-up lemmatizer wnlet = WordNetLemmatizer() #Iterate through seasons for s in range(1,9): season_words[s] = [] for obj in diag: #Iterate through all object in dialogoue if obj[&#39;seasonNum&#39;] == s: #Only use elements from the correct season #Get text words = [sentence[&#39;text&#39;] for sentence in obj[&#39;text&#39;]] #Tokenize, remove stopwords, lemmatize words: word_tokens = wordpunct_tokenize(&quot; &quot;.join(words)) filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] filtered_sentence = [w for w in filtered_sentence if w != &quot;...&quot;] words = [wnlet.lemmatize(w.lower()) for w in filtered_sentence] #Save the words: season_words[s].extend(words) . As the dispersion_plot function by nltk does not allow custom x-ticks we have modified the original function to handle this. Further, we have modified the function so the default grid-lines in the horizontal direction better illustrates when a season ends and stops. This now allows us to display how the selected words are used season by season, and hopefully makes it easier to grasp. . def dispersion_plot(text, words, xticks = None, xlabels = None, lines = None): &#39;&#39;&#39; We have modified the dispersion plot function by NLTK to include custom x-ticks, so we can display how the use of selected words changes through the series. Further we have changed the default lines created by sns.set(), to better display when one season stops and another starts. &#39;&#39;&#39; text = list(text) words.reverse() points = [ (x, y) for x in range(len(text)) for y in range(len(words)) if text[x] == words[y] ] if points: x, y = list(zip(*points)) else: x = y = () #Add vertical lines if lines is not None: for l in lines: pylab.plot([l,l], [-1,len(words)], color = &#39;white&#39;) #Plot the points pylab.plot(x, y, &quot;b|&quot;, scalex=0.1, markersize = 12) #Y-ticks ( ie. words) pylab.yticks(list(range(len(words))), words, color=&quot;b&quot;, fontsize = 12) pylab.ylim(-1, len(words)) pylab.title(&quot;Lexical Dispersion Plot&quot;, fontsize = 20) pylab.xlabel(&quot;Season offset&quot;, fontsize = 12) #Add x-tciks if xticks is not None: pylab.xticks(ticks=xticks, labels=xlabels, rotation = &#39;45&#39;, fontsize = 12) plt.tight_layout() plt.grid(axis = &#39;x&#39;) pylab.show() . We will start out by defining the x-ticks and their location which we do by iterating through our season_word dict and defining the location of the x-ticks as the center of this season. The season length are defined by the number of words in this season. Therefore the length can vary based on the length of the dialogoue. Further, we save the end/start of each season so we can draw vertical lines here when plotting. . all_words = [] for words in season_words.values(): all_words.extend(words) ticks = [] labels = [] vert_line = [0] tmp = 0 #Get x-tick position, labels and vertical line positions for i, (key, value) in enumerate(season_words.items()): ticks.append(int(np.round((tmp + (tmp+len(value)))/2))) labels.append(&#39;Season &#39; + str(key)) tmp += len(value) vert_line.append(tmp) . We are now ready to visualize how the words develop across the series and the selected words are: . white and walker as we want to investigate how often white walkers are metioned across the series, as they are an underlying theme, together with the sentence winter is coming which is the reason we have chosen the word winter as well. . A little in the same category is wildling and these people get to have a large impact later in the series, and are therefore also chosen. . Next, we want to look into wedding as many characters are married across allegiances, cultures and religions, and play a big part of the story line, but as well the notorious Red Wedding which we want to look into. . We have further chosen khaleesi and dragon as Daenerys Targaryen and her dragons play a large role in the story and do set the scene multiple times throughout the season. . Wolf is also a chosen words, as it both symbolizes the Stark family, but also the wolfs they get as children when they find the mother wolf murdered. . Lastly, two big themes in the story namely murder many get killed throughout the series and debt which is one of Tyrion Lannisters main sentences namely: A Lannister always pays his debts. . The resulting lexical dispersion plot can be seen below. . plt.figure(figsize = (12,8)) dispersion_plot(all_words, [&quot;winter&quot;,&quot;walker&quot;,&quot;white&quot;,&quot;wildling&quot;,&#39;wedding&#39;,&#39;khaleesi&#39;, &quot;dragon&quot;,&quot;wolf&quot;, &quot;murder&quot;,&quot;debt&quot;], xticks = ticks, xlabels = labels, lines = vert_line) . Looking at the lexical dispersion plot above, the first word we chose was winter. This is due to the famous Stark house words being &quot;Winter is coming&quot; and we wanted to investigate how much this phrase was actually used. It appears winter is most used in the beginning and the end of the show. Only sorting for the word winter has the caveat though, that other common phrases such as the long winter are also represented here. . Another interesting comparison is the words dragon and wolf. Both the Targaryens and Starks are refered to as dragons and wolves respectivly but the Stark children also raise their own dire wolf throughout the show. The same can be said for Daenarys whose dragons are born in the end of season 1 and raised througout the show. In the beginning of the show, the wolves are more commonly mentioned compared to the ending where they are barely mentioned. The opposite holds true for dragon which is less mentioned in the beginning but mentioned more and more as the story unfolds. . It can also be seen that the word wedding is mentioned most during season 3 and season 4. This holds true to the story as both Robb Stark, Joffrey Baratheon and Sansa Stark are all married during these seasons. . 4. Subconclusion . Through the use of wordclouds it was possible to dive into the words used by a few selected characters and allegiances, and the extracted words clearly did explain these characters and allegiances. The words did also correspond well with the biased image of the characters and allegiances. Further, through the TF-IDF analysis of the seasons, we could see that the theme clearly changes as the series progress, and this was further backed up by the dispersion plot. . Lastly, by analysing the sentiment of the characters based on the dialogoue and wiki-pages that the dialogoue did contain larger variation in sentiment which would be expected as dialogoue would be expected to contain more polarizing words. . &lt;/div&gt; .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/Explainer_Text/",
          "relUrl": "/Explainer_Text/",
          "date": ""
      }
      
  

  
      ,"page8": {
          "title": "Explainer Notebook - Community Analysis",
          "content": "Community Analysis . This section will go through the community analysis in this project. This section includes a description of how the communities were detected and exploratory analysis of these. The exploratory analysis are going to focus on three of the character attributes namely allegiance, religion and culture, as these appears to be the most important. Next we are going to perform text analysis which will include computation of TF-IDF which is presented in wordclouds. Lastly, are we going to look at the community sentiment. . Before starting on the analysis are we going to import the needed packages. . import plotly.graph_objects as go from plotly.subplots import make_subplots import pandas as pd import numpy as np import networkx as nx import matplotlib.pyplot as plt import urllib import json import re import os from imdb import IMDb import contextlib import powerlaw from fa2 import ForceAtlas2 import powerlaw from wordcloud import WordCloud from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer import requests import nltk from nltk import FreqDist from nltk.text import TextCollection from nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizer from community import community_louvain import plotly.express as px from itertools import groupby #Download NLTK stuff for lemmatizer and tokenizer: nltk.download(&#39;punkt&#39;) nltk.download(&#39;wordnet&#39;) #Set stuff for nice formmatting of plots: import seaborn as sns sns.set() %matplotlib inline import itertools nltk.download(&quot;stopwords&quot;) from nltk.corpus import stopwords from nltk.tokenize import wordpunct_tokenize import string . [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Unzipping corpora/wordnet.zip. [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. . 1. Community detection and exploratory analysis . . This section will start by detecting the communities in the Game Of Thrones network across all seasons, and how the network is generated can be seen in section 2.1.1 in the Explainer Notebook. Afterwards, are we going to investigate the communities. We will start out by reading the network file which is saved in a gpickle file. . G = nx.read_gpickle(&quot;../data/got_G.gpickle&quot;) . 1.1 How the communities are detected . We are going to detect communities based on the Louvain algorithm. The Louvain algorithm works in a two phased manner which is a modularity optimization phase and a community aggregation phase. Starting out with the modularity optimization here every node has its own community, afterwards it is estimated what the potential change in modularity is if the node is moved in two the neighbouring communities. The node is then moved two the community which has the highest increase ie. improvement of modularity. The node is staying in the community if the modularity does not improve. . After optimizing the modularity it is time for phase two where the community aggregation begin. Here all nodes assigned to the same community are merged to one node. Inter-community edges are turned into selfloop, and the selfloop is weighted by the number of edges in the given community. . The phases are repeated in a iterative manner until the network achieve no further improvement in modularity. . As the algorithm needs a undirected network we are going to convert our directed network to a undirected using the built-in function to_undirected. . Afterwards we order our communities in a dictionary, where the keys are the character names and the values are the character names. This will make the investigation of the communities more easy later on. . G_unDi = G.to_undirected() partition = community_louvain.best_partition(G_unDi, randomize=False) #Create a dict containing the communities and the characters in the community: link_partition = {c: [character for character, community in partition.items() if community == c] for c in set(partition.values())} . Having generated the network are we going to investigate some basics stats of these communities namely how many communities have we detected and what is the modularity of this partitioning. . print(f&#39;Number of communities in network: {len(link_partition)}&#39;) print(f&#39;Modularity of partition: {round(community_louvain.modularity(partition,G_unDi),3)}&#39;) . Number of communities in network: 6 Modularity of partition: 0.319 . The Louvain algorithm has detected 6 communities, and this partitioning have a modularity of 0.319 which indicates that the partitioning clearly contains sub-groups which the algorithm have detected. . Having determined that the split is decent and we have 6 communities we are going to look at the size of each community. . dict_list = {} #Save the community sizes dict_list[&quot;Number of members in community&quot;] = [len(com) for com in link_partition.values()] #Save community number dict_list[&quot;Community&quot;] = list(link_partition.keys()) #Convert to dataframe for easy plotting df = pd.DataFrame(dict_list) #Show plot fig = px.bar(df, x=&quot;Community&quot;,y =&#39;Number of members in community&#39;, title = &quot;Distribution plot of community sizes&quot;) fig.show() . . . Clearly community 0 is the smallest with only 13 characters whereas community 1 is the largest community with 43 characters. Community 2-5 is quite similar in their sizes and contain in the range of 25-29 characters. When thinking of the largest group of people community 1 could come from the group of characters living in Kings Landing. . 1.2 Exploratory analysis of communities . Having determined community sizes, we are going to dig a little deeper into each community and its characteristics, namely how does the distribution of religions, allegiances and cultures look inside each community. This could help us decode which characters are present and how the community has been created. This could point us in the right direction of what is the underlying pattern. . attributes_dict, dfs, figs = {},{},{} #Selected attributes attributes = [&quot;religion&quot;, &quot;appearances&quot;, &quot;culture&quot;, &quot;allegiance&quot;] #Iterate through the selected attributes: for attribute in attributes: #Get all attributes from the network attributes_dict[attribute] = nx.get_node_attributes(G, attribute) com_dict = {} att,counter,com_ = [], [],[] #Iterate through all communities for com, char_list in link_partition.items(): #Extract the attribute values for each character: tmp = [attributes_dict[attribute][char] for char in char_list] #Extract count of categories for each attribute tmp2 = [[tmp.count(att),att]for att in set(tmp) if att != &quot;&quot;]# #Parse empty strings to unknown for l1 in tmp2: if l1[1] == &quot;&quot;: l1[1] = &quot;Uknown&quot; att.append(l1[1]) counter.append(l1[0]) com_.extend([com]*len(tmp2)) #Generate dataframe and remove nans dfs[attribute.capitalize()] = pd.DataFrame({&quot;Community&quot; : com_, attribute.capitalize() : att, &quot;Count&quot; : counter}).fillna(0) #Create figure and save in dict: figs[attribute.capitalize()] = px.bar(dfs[attribute.capitalize()], x=&quot;Community&quot;, y=&quot;Count&quot;, color=attribute.capitalize(), title=&quot;{} distribution in communities&quot;.format(attribute.capitalize()), width = 900, height = 375) . We have now generated the figures for each of the selected attributes and saved them in a dictionary so we can visualize them one-by-one and trying to understand the pattern. . figs[&#39;Religion&#39;].show() . . . From the above we can notice that Community 0 contains equal parts of characters being part of Faith of the Seven religion and Drowned God, whereas Community 1 mainly contain characters being part of Faith of the Seven. . Community 0 could therefore be expected to contain people from the Iron Island and as well from Kings Landing, whereas Community 1 contains primarily people from Faith of the Seven, and 1 from the Old Gods of the Forest, this could maybe be people being from House Lannister and Sansa Stark as these characters are very well connected in large parts of the story. . Next, in Community 2 the religions are a mix of the Great Stallion, the Old Gods of the Forest, the Faith of the Seven and Ghiscari. This could be due to the characters surrounding Daenarys Targaryen, as her group are a rather mixed group. . The next three communities are also rather mixed, and could represent multiple groups of characters thus being rather complicated to decrypt. . figs[&#39;Allegiance&#39;].show() . . . From the figure above it can be seen that the communities are very mixed when it comes to allegiance, which would be expected as the Game Of Thrones universe contains a lot of different allegiances, and these are mixing up together. Eg. the people from the North are part of many different allegiances, but are grouping together when needed for eg. war. . One could see that the largest allegiance in community 1 are House Lannister, but also contains Petyr Baelish, Kingsguard, Sparrows, House Clegane etc. This community appears to describe the important people of Kings Landing. . Community 2 is described by House Targaryen, Second Sons, Unsullied, Drogo and could describe the group sorounding Daenarys Targaryen. . Community 3 is described by House Umber, Free Folk, Night&#39;s Watch and White Walkers, maybe this community describes the people interacting at the Wall and the land around it. This could explain this group of allegiances. . figs[&#39;Culture&#39;].show() . . . Lastly, we are looking into what culture each community contains. Here it can be seen that the Andals are the largest in Community 1, 3, and 5. This would be expected as this is the largeset cultural group in Game Of Thrones. . Community 2 contains large group of Dothraki people, and this again underlines that community 2 could be the group of characters surrounding Daenarys. . Similarly, we see that community 3 contains Free Folk, Northmen, White Walkers, Children of the Forest, which could indicate the people at the Wall and beyond. . 2. Network analysis of communities . This section is devoted to investigate the each of the community networks, which is going to be presented in an interactive visualization like the one in the Network Analysis notebook visualizing the network of each season and across them all. This app have the same fucntionalities. . We are going to look a little deeper into what each community contain namely what characters, as we have already looked into the distribution of the character attributes. . 2.1 How is the app created . The app has the same setup as the app described in Network Analysis, where the node size and the alpha value ie. how transparent each node is depends on its degree. Further, the edge width depends on the frequency of the link between the two given nodes. The positions of the nodes are determined by the ForceAtlas2 algorith and the specific settings can be seen here. . The functionality of the app are the same where the user have four sections in the bottom. To the left it is possible to select the community one wants to investigate. Just to the right one can select the overlay namely religion, culture and allegiance, which changes the colour of the nodes. . If one selects a node by clicking on it, further information of the selected node will appear in the Click data section, and the image of the selected character in Character image section. . The app is hosted on a separate Github as Heroku only allows for one app pr. repository. The app uses as input the full network gpickle file and as well the link partitioning dictionary which generate above. By selecting a community the characters from the original network is extract with all the information ie. attributes and etc. which then are displayed below. . The app contains two separate python files, one for processing the data which can be found here and one for generation of the app which can be found here. The repository containing all the data and files for the app generation can be found here. . 2.2 Insights . We are now going to extract some insights from the app which were described above, and trying to understand which characters are present in each community. . Community 0: Lets start with community 0. This community is mostly based on characters from the Sand family, Martell and Greyjoy and last Myrcella Baratheon. Where it appears that Ellaria Sand is the key person. First of all the Martell and Sand famiilies are strongly connected as Oberyn and Ellaria is married, further the plan is that Myrcella Baratheon should be married with Trystane Martell. And clearly, Ellaria is playing a key role in all these interactions after Oberyn are killed by Gregor Clegane. . Community 1: This community clearly have all the characters acting in King&#39;s Landing this includes Paetyr Baelish, Sansa Stark, Cersei Lannister, Tyrion Lannister, Shagga, Shay and Meryn Trant just to name a few. Which all acts here, Shagga comes to King&#39;s Landing with Tyrion Lannister with the promise of getting the River Lands. Clearly, Cersei, Sansa, Tyrion and Petyr plays a large role in this community as these are a big part of the story and interacts with tons of other characters. . Community 2: This community is clearly generated based on the characters sorounding Daenerys Targaryen which also are key element in this community which can seen from the app. Further, Jorah Mormont are a key player and also linking her with some of the people of Westeros. To name a few characters sorounding her is Drogo, Viserys and Robert Baratheon. Drogo is her husband until he dies, but still have a large impact on her story moving forward as she gets her dragons while being married with him. Viserys is her brother, and are &quot;travelling&quot; with her in the first season. Robert tries to get her murdered as the Targaryen family are attempted to eradicate. . Community 3: Community 3 clearly has Jon Snow in focus, and the people sorounding him and to name just a few: Samwell Tarly, Gilly, Ygritte, Mance Raider and Maester Aemon. Samwell becomes one of Jons best friends while being part of the Night&#39;s Watch, and he finds Gilly which we also can see they are very well connected. Further, when Jon becomes friends with the wild Mance and Ygritte become important characters for Jon Snow. Aemon are Maester in the Night&#39;s watch and are an important person for both Samwell and Jon which we also can see from the network. . Community 4: Robb Stark is the main character in this community. He is impacted by the people involved in the war and his story including Bolton family, his mother Catelyn Stark and Walder Frey. . Community 5: This community clearly has Arya Stark in center, and people impacting her story. This includes Jaqen H&#39;ghar, Gendry Baratheon, Hot Pie and Sandor Clegane. Jaqen is the man with many faces learning Arya how to fight in the end after making her blind. Gendry travels with Arya when leaving King&#39;s Landing after Ned Stark are killed. Sandor Clegane assists Arya multiple times. Further we can see that Gendry are connected with a little group sorounding Davos. . 3. Text analysis of communities . Having determined the properties of the communities and which character there are present we are next diving into text analysis of the communities. The text analysis consist of computing TF-IDF for each community, which generally have the samme approach as in the Text Analysis Notebook. . Before starting on this we are going to define a function for the final cleaning of the text. . The text used for the text analysis in this part is based on the wiki-pages of each character. . def clean_text(txt, charlist): &quot;&quot;&quot; Function for cleaning the text. As input it takes the text that needs cleaning and a character list of which character name that should be removed. The function tokenize, remove stopwords, lemmatize and turn the text into lower case. &quot;&quot;&quot; #Convert text to lower case txt = txt.lower() #Tokenize: word_tokens = wordpunct_tokenize(txt) #Remove stopwords filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #Lemmatize text wnlet = WordNetLemmatizer() words = [wnlet.lemmatize(w) for w in filtered_sentence] return words . We now need to create a list of character names including full name, first name and last name which we wan&#39;t to remove from the text. We append the names to the list of stopwords. . char_list = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] char_names = [] for char in char_list: #Turn name to lower char = char.lower() #Get full name char_names.append(char.replace(&quot;_&quot;,&quot; &quot;)) #Get first and last name char_names.extend(char.split(&#39;_&#39;)) #Get all stopwords stop_words = set(stopwords.words(&#39;english&#39;) + list(string.punctuation) + list(char_names)) . First we are going to extract all the text needed for the text analysis. The text can be found here, where we are going to load all text files for all characters and save them in a dictionary. The dictionary keys are the character names and the values are the cleaned text using the clean function described above. . char_pages = {} #Get all characters: characters = [f.split(&#39;.txt&#39;)[0] for f in os.listdir(&quot;../data/got_cleaned/&quot;)] #Iterate through all characters for char in characters: #Replace _ with space name = char.replace(&#39;_&#39;, &#39; &#39;) #Load text with open(&#39;../data/got_cleaned/&#39;+char+&quot;.txt&quot;, &quot;r&quot;, encoding = &#39;utf-8&#39;) as text_file: txt = text_file.readlines() #Clean text and save in dict char_pages[name] = clean_text(&quot; &quot;.join(txt),characters) . As we have loaded the data in a dictionary for each character it is now easy to compute the TC for each community, as we can just iterate through each community and the characters in the given community. When iterating through the characters in the community we just append the text for the selected community. To compute the frequencies of the words use the FreqDist function form the nltk library. . l_dict_com, tc_dict_com = {}, {} for com, characters in link_partition.items(): words = [] for char in characters: try: words.extend(char_pages[char.replace(&quot;_&quot;, &quot; &quot;)]) except: continue l_dict_com[com] = len(words) #Compute TC for the community words: tc_dict_com[com] = FreqDist(words) . The TC are computed and saved in a dictionary where the keys are the community number and the values are the frequencies of the words in the given community. . We are now ready to compute the next part of the TC-IDF score namely the IDF. We have all the unique words across all communities in the TC-dictionary and we therefore extract all unique words from this utilizing the set functionality in python which only save unique values. The last component before we can compute the IDF are the number of documents, which is the number of communities ie. 6. We are now ready to compute the IDF in same manner as described in the Text Analysis Explainer Notebook. . unique_words = set(list(itertools.chain(*tc_dict_com.values()))) # Calculate idf idf_com = {word: #Find in how many documents that each word is present: np.log(len(link_partition.keys()) / sum([tf[word] &gt; 0 for k, tf in tc_dict_com.items()])) #Iterate through unique words: for word in unique_words } . We have now a dictionary with the IDF values of all words saved in a dictionary with keys being the words and the values the IDF value of the given word. We are now just left with the task of computing the TF-IDF score which can easily be computed by iterating through all the communities and then the words in the given community. This is done below. . tf_idf_com = {} #Iterate through communities: for com in link_partition.keys(): tf_idf = {} #Iterate through each word in each community: for word, tf_value in tc_dict_com[com].items(): #Extract IDF idf_value = idf_com[word] #Compute TF-IDF tf_idf[word] = idf_value*tf_value/l_dict_com[com] tf_idf_com[com] = tf_idf . The TF-IDF values are now ready for all words in each community. We are going to visualize this using a plot function and by the use of the WordCloud function. The function for plotting can be seen below. . def plot_wordcloud(communities,tc_idf_com, name=None): &quot;&quot;&quot; Function for plotting the wordclouds of a TC-IDF dict. &quot;&quot;&quot; #Setup figure plt.figure(figsize = (12,15)) #Iterate through communities: for i,com in enumerate(communities): word_list= [] #Get all words for the community for word, value in tc_idf_com[com].items(): word_list.append((word+&#39; &#39;)*int(np.ceil(value))) #Make list of words to string word_list = &quot; &quot;.join(word_list) #Generate wordcloud wc = WordCloud(collocations=False).generate(word_list) #Plotting plt.subplot(3,2,i+1) plt.imshow(wc, interpolation=&#39;bilinear&#39;) if name is not None: com = name[i] plt.title(com, fontsize = 40) plt.axis(&quot;off&quot;) plt.tight_layout() plt.show() . We are now ready to visualize our text analysis in the wordclouds which are presented in the figure below. . selected_com = [0,1,2,3,4,5] #Change community name names = [&quot;Community &quot; + str(com) for com in selected_com] #Plot communities plot_wordcloud(selected_com, tf_idf_com, names ) . We notice that the words are very descriptive. If we use community 2 as an example we can see that these words match very well with the expectation that this group describes the people around Daenerys Targaryen. This includes khal, khalasar, invade, conquer and reclaim as she starts out with being wife of Khal Drogo and being part of his khalasar. Later she invades multiple cities and conquer a large part of the Game Of Thrones universe. Further she tries to reclaim the Iron Throne. . The same pattern of words describing the community can be seen in the other wordclouds. . 4. Sentiment analysis . This part will dive into the sentiment of each community, this will be based on character dialogoues instead of the wiki-pages as used in the previous part. This is done as it is expected that the dialogoue of each character better resembles the character&#39;s mood rather than a wiki-page describing the character. . The principles behind LabMT and VADER are described in the Text Analysis Explainer Notebook. . We will start out by loading the LabMT data, and convert this into a dictionary where we have the word as key and the happiness score as value. . LabMT = pd.read_table(&#39;../data/labMIT-1.0.txt&#39;, delimiter=&quot; t&quot;) #Convert LabMT to a dictionary: LabMT_dict = {word : happiness_score for word,happiness_score in zip(LabMT[&#39;word&#39;], LabMT[&#39;happiness_average&#39;]) } . Next, we load the VADER sentiment analyzer using the built-in function SentimentIntensityAnalyzer and lemmatizer. . analyzer = SentimentIntensityAnalyzer() lemmatizer = WordNetLemmatizer() . We now define two functions for computing the LabMT and VADER sentiment scores of a list of tokens. Where these returns the mean of the sentiment of the tokens it takes as input. . def sentiment_LabMT(tokens): #Extract tokens that are present in LabMT: tokens_LabMT = [token for token in tokens if token in LabMT_dict.keys()] #Extract sentiment values of tokens: happiness_LabMT = [LabMT_dict[token] for token in tokens_LabMT] #Return mean values of sentiments: return np.mean(happiness_LabMT) def sentiment_VADER(tokens): happiness_VADER = [analyzer.polarity_scores(sentence)[&#39;compound&#39;] for sentence in tokens] return np.mean(happiness_VADER) . The data are now loaded from Github in json format, where we are going to save it in a new format. Here we are going to save it in a character dictionary, where the keys are the character name and values are the character dialogoue. . resp = requests.get(&quot;https://raw.githubusercontent.com/jeffreylancaster/game-of-thrones/master/data/script-bag-of-words.json&quot;) diag = json.loads(resp.text) char_diag = {} for element in diag: episode = element[&#39;episodeNum&#39;] season = element[&#39;seasonNum&#39;] title = element[&#39;episodeTitle&#39;] text = element[&#39;text&#39;] #Iterate through all dialogoue for the episode: for textObj in text: #Save the dialogoue for the character if textObj[&#39;name&#39;] in char_diag: char_diag[textObj[&#39;name&#39;]].append(textObj[&#39;text&#39;]) else: char_diag[textObj[&#39;name&#39;]] = [textObj[&#39;text&#39;]] . As this data contain more characters than we have in our network we are going to create a list with all the characters that we have in our character network. . files = os.listdir(&quot;../data/got/&quot;) #Get character names: char_list = [file.split(&#39;.txt&#39;)[0] for file in files] . As VADER does not expect any preprocessing of the text we are just going to give the raw text. The LabMT methods expects the text have been preprocessed. We are therefore going to tokenize and lemmatize the text and convert into lower case. . tokens_LabMT = {char : [lemmatizer.lemmatize(word) for word in word_tokenize(&quot; &quot;.join(text).lower())] for char, text in char_diag.items()} tokens_VADER = char_diag . Now we have the tokens ready for the two sentiment functions defined above, and we therefore compute the sentiment for all characters that are present in our network. This is done below: . warnings.filterwarnings(&quot;ignore&quot;) #Compute sentiment for each character: char_sentiment_LabMT = {char :sentiment_LabMT(tokens_values) for char, tokens_values in tokens_LabMT.items() if char.replace(&quot; &quot;, &quot;_&quot;) in char_list} char_sentiment_VADER = {char :sentiment_VADER(tokens_values) for char, tokens_values in tokens_VADER.items() if char.replace(&quot; &quot;, &quot;_&quot;) in char_list} . We now have two dictionaries with the average sentiment for each character saved in easy to access dictionaries. It is now easy to compute the average sentiment for each community. We can just iterate through each community and the characters in each community and take the average of the sentiment of the characters in the community. The same approach is taken for computing the standard deviation of the sentiment of each community. . As some characters might not have a sentiment value as they maybe don&#39;t have any dialogoue we utilize nanmean and nanstd from the numpy library which ignores nan values. . com_sentiment_VADER = {com : #Compute sentiment for community: np.nanmean([char_sentiment_VADER[char.replace(&quot;_&quot;, &quot; &quot;)] for char in characters if char.replace(&quot;_&quot;,&quot; &quot;) in char_sentiment_VADER.keys()]) #Iterate through each community: for com, characters in link_partition.items()} com_sentiment_LabMT = {com : #Compute sentiment for community: np.nanmean([char_sentiment_LabMT[char.replace(&quot;_&quot;, &quot; &quot;)] for char in characters if char.replace(&quot;_&quot;,&quot; &quot;) in char_sentiment_LabMT.keys()]) #Iterate through each community: for com, characters in link_partition.items()} #Compute standard deviations of the sentiment for each community: com_sentiment_VADER_sd = {com : np.nanstd([char_sentiment_VADER[char.replace(&quot;_&quot;,&quot; &quot;)] for char in characters if char.replace(&quot;_&quot;,&quot; &quot;) in char_sentiment_VADER.keys()]) for com, characters in link_partition.items()} com_sentiment_LabMT_sd = {com : np.nanstd([char_sentiment_LabMT[char.replace(&quot;_&quot;,&quot; &quot;)] for char in characters if char.replace(&quot;_&quot;,&quot; &quot;) in char_sentiment_LabMT.keys()]) for com, characters in link_partition.items()} . The last part is visualizing the average sentiment for each community and their respective standard deviation. This is done through the use of bar plots and adding error bars. The visualization is done in plotly to make it more appealing and interactive. The function below is written for the plotting. . def plot_VADER_LabMT_scores(char_sentiment_VADER,char_sentiment_LabMT, error_bar = False, com_sentiment_VADER_sd=None, com_sentiment_LabMT_sd=None, x_text = &quot;Characters&quot;): # Create figure with secondary y-axis fig = make_subplots(rows=1, cols=2)#,specs=[[{&quot;secondary_y&quot;: True}]]) #Two settings one without error bars and one with: if error_bar: # Add traces fig.add_trace( go.Bar(x=[*char_sentiment_LabMT],#Community names y=list(char_sentiment_LabMT.values()), #Average sentiment values error_y=dict( #Adding std in error bars type=&#39;data&#39;, array= list(com_sentiment_LabMT_sd.values()), visible=True),name=&quot;LabMT score&quot;), row=1, col=1, #plot location in subplot ) fig.add_trace( go.Bar(x=[*char_sentiment_VADER], y=list(char_sentiment_VADER.values()), error_y=dict( type=&#39;data&#39;, array= list(com_sentiment_VADER_sd.values()), visible=True), name=&quot;VADER score&quot;), row=1, col=2, ) else: fig.add_trace( go.Bar(x=[*char_sentiment_LabMT], y=list(char_sentiment_LabMT.values()), name=&quot;LabMT score&quot;), row=1, col=1, ) fig.add_trace( go.Bar(x=[*char_sentiment_VADER], y=list(char_sentiment_VADER.values()), name=&quot;VADER score&quot;), row=1, col=2, ) # Add figure title fig.update_layout( title_text=&quot;LabMT and VADER sentiment of communities&quot; ) # Set x-axis title fig.update_xaxes(title_text=x_text) # Set y-axes titles fig.update_yaxes(title_text=&quot;&lt;b&gt;LabMT sentiment score&lt;/b&gt;&quot;, row=1,col = 1) fig.update_yaxes(title_text=&quot;&lt;b&gt;VADER sentiment score&lt;/b&gt; &quot;, row = 1, col = 2) return fig . The sentiment result for each community can now be presented in the figure below. . fig = plot_VADER_LabMT_scores(char_sentiment_VADER= com_sentiment_VADER, char_sentiment_LabMT = com_sentiment_LabMT, error_bar = True, com_sentiment_VADER_sd = com_sentiment_VADER_sd, com_sentiment_LabMT_sd= com_sentiment_LabMT_sd, x_text = &quot;Community&quot;) fig.show() . . . The figure above displays the computed sentiment values for each community based on LabMT and VADER sentiment methods, further the standard deviation are displayed as an error-bar, which can indicate whether a community has a large variation in sentiment values. It can be seen that based on LabMT all communities have almost the same sentiment level and rather small variation, though community 1 has the largest variation, which also can be seen from the VADER sentiments. . The VADER sentiment appear to have a little larger variation between the communities and as well within each community. . The VADER score ranges from -4 to +4 and a score of zero indicates that the sentiment is neutral. From this it can be seen that community 2 and three are slightly positive, whereas community 1 are more positive but not happy. Further community 5 appears to be the most sad community. When looking at the LabMT score it ranges from 1.3 to 8.5 where a score of 5 is neutral. Again all the communities are close to being neutral where community 1 is the most happy and community 3 and 5 are the saddest which corresponds with the results from the VADER sentiment scores. . From the sentiment analysis of each community we find that the most happy community is Community 1 and 4 whereas the sadest are Community 5 and 3. . 5. Subconclusion . The Louvain algorithm did extract six communities, and through the exploratory analysis it became clear that the characters were not grouped by their attributes (allegiance, religion etc.) but instead based on which people they were sourounded by. Daenarys Targaryen was a good example of this. Further when we looked into the community networks this became even more clear that this was the underlying pattern connecting these characters. Which seemed obvious when you think about it. We further analyzed the words of each community and this further backed up the hypothesis, and lastly the sentiment analysis that these communities clearly had different levels of sentiment. .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/Explainer_Community/",
          "relUrl": "/Explainer_Community/",
          "date": ""
      }
      
  

  

  
      ,"page10": {
          "title": "",
          "content": ". of the website . The website contains 6 sections. The first section The Data describes the data, where it is found, and briefly what it is used for. The next section performs an initial exploratory analysis of the data. The third section performs an analysis of the Game Of Thrones networks describing how the network is created and how the charaters interact. The next part performs text analysis of the character dialogoues but also the text from the character wiki-pages. The fifth section detects communities in the network, and perform analysis on these. . Lastly, the analysis performed throughout the website can be found in the explainer notebook if one want&#39;s to dig into all the details. If one want&#39;s to replicate the analysis the data can be downloaded in the data section. . The Data | Basic Statistics of the Data | The Game of Thrones Network | Text Analysis | Community Detection | Explainer Notebook | . Aim of project . The Game of Thrones series recieved world-wide rekognition and is the most watched series to date. It takes place in a mediaval fantasy universe and introduces many characters with complicated relations. The goals of this project is to try and answer, who was actually the main character throughout the seasons and if it is possible to find a pattern in all these character relations. We will also investigate the overall themes of the series and if these changes over the course of the show. . To sum it up, this project will try to answer the following: . Who is the main characters of each season of the series? | Is it possible to find a pattern in the data that helps understand the complicated world of Westeros? | Is the theme of the Game of Thrones series consistent throughout the show or does it change during it&#39;s course? | . The following video is a short presentation of the aim of the project: . . The Game of Thrones Universe . The Game of Thrones is based on the book series &quot;A Song of Ice and Fire&quot; written by George R. R. Martin and takes place in the mediaval fantasy world of Westeros and Essos. During the show, the viewer follow multiple story arcs: A succesion war amongst the houses of Westeros as the king, Robert Baratheon, dies and doubt of the legitimacy of his childrens claim to the throne arises. Another arc takes place in the northern most part of Westeros, where the Night Watch have guarded the Wall keeping the Seven Kingdoms safe for centuries. And a third arc, where the viewers follow an exiled princess who wishes to reconquer The Seven Kingdoms and reclaim her birth right, the Iron Throne. . . The Website . This website is built using the blogging template fastpages built by fastai. It uses GitHub actions to create Jekyll blog posts on GitHub Pages by converting jupyter notebooks to html .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page18": {
          "title": "",
          "content": "Sitemap: {{ sitemap.xml | absolute_url }} | .",
          "url": "https://mikkelmathiasen23.github.io/GameOfThrones_Network/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}